# STORY-0001.4.4: Safetensors Compression with zstd

**Parent Epic**: [EPIC-0001.4](../README.md)
**Status**: ðŸŸ¡ In Progress
**Story Points**: 2
**Progress**: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 75%

## User Story

As a developer with a large repository
I want embedding cache compressed efficiently
So that .gitctx/ directory size is minimal and git operations remain fast

## Acceptance Criteria

- [ ] Embedding cache uses `.safetensors.zst` format (zstd compressed)
- [ ] Compression level: 3 (balance of speed and ratio)
- [ ] Cache size: ~11MB for 100 files with ~8% compression (vs ~12MB uncompressed safetensors, ~60MB JSON)
- [ ] Compression ratio: ~8-10% size reduction for typical embedding data (float32 arrays with moderate entropy)
- [ ] Decompression transparent to EmbeddingCache API (no caller changes)
- [ ] Backward compatibility: None (no users yet, clean migration)
- [ ] Compression/decompression performance: <10ms overhead per file
- [ ] Unit tests verify compression ratio achieves ~8-10% size reduction for typical embedding data

## BDD Scenarios

**Note:** Compression is an implementation detail (not user-observable behavior). BDD scenarios minimal, focus on unit tests.

### Scenario: Embedding cache compressed on disk

```gherkin
Given I index a repository with 100 files
When I check the .gitctx/embeddings/ directory
Then cache files should have .safetensors.zst extension
And total cache size should be ~11MB (8% smaller than uncompressed 12MB)
```

### Scenario: Decompression transparent to search

```gherkin
Given I have indexed a repository with compressed cache
When I search for "authentication"
Then search results should be identical to uncompressed cache
And decompression should add <100ms to search time
```

## Technical Design

### Components to Modify

**1. `src/gitctx/storage/embedding_cache.py`** (Modify existing)

Update `get()` and `set()` to use zstd compression:

```python
import json
import struct
import zstandard as zstd
from pathlib import Path
from safetensors.numpy import save, load

class EmbeddingCache:
    """Embedding cache with zstd compression."""

    COMPRESSION_LEVEL = 3  # Balance of speed and ratio

    def get(self, blob_sha: str) -> list[Embedding] | None:
        """Load cached embeddings with transparent decompression."""
        path = self.cache_dir / f"{blob_sha}.safetensors.zst"
        if not path.exists():
            return None

        try:
            # Decompress
            compressed = path.read_bytes()
            dctx = zstd.ZstdDecompressor()
            decompressed = dctx.decompress(compressed)

            # Extract metadata from safetensors header
            # (Standard safetensors pattern for bytes - see https://huggingface.co/docs/safetensors/metadata_parsing)
            header_size = struct.unpack('<Q', decompressed[:8])[0]
            json_header = decompressed[8:8+header_size].decode('utf-8')
            header = json.loads(json_header)
            metadata = header.get('__metadata__', {})

            # Load tensors from decompressed bytes
            tensors = load(decompressed)

            # Reconstruct Embedding objects from tensors + metadata
            embeddings = []
            chunk_count = int(metadata.get("chunks", 0))
            for i in range(chunk_count):
                tensor = tensors[f"chunk_{i}"]
                vector = tensor.tolist()
                embeddings.append(
                    Embedding(
                        vector=vector,
                        token_count=int(metadata.get(f"chunk_{i}_tokens", 0)),
                        model=self.model,
                        cost_usd=float(metadata.get(f"chunk_{i}_cost", 0.0)),
                        blob_sha=blob_sha,
                        chunk_index=i,
                    )
                )
            return embeddings

        except Exception as e:
            logger.warning(f"Failed to load cache for {blob_sha[:8]}: {e}")
            return None

    def set(self, blob_sha: str, embeddings: list[Embedding]) -> None:
        """Save embeddings with zstd compression."""
        path = self.cache_dir / f"{blob_sha}.safetensors.zst"

        # Convert to tensors and metadata
        tensors = {}
        metadata = {}
        for i, emb in enumerate(embeddings):
            tensors[f"chunk_{i}"] = np.array(emb.vector, dtype=np.float32)
            metadata[f"chunk_{i}_tokens"] = str(emb.token_count)
            metadata[f"chunk_{i}_cost"] = str(emb.cost_usd)

        metadata["model"] = self.model
        metadata["blob_sha"] = blob_sha
        metadata["chunks"] = str(len(embeddings))

        # Serialize to bytes (NOT file)
        safetensors_bytes = save(tensors, metadata=metadata)

        # Compress with zstd
        cctx = zstd.ZstdCompressor(level=self.COMPRESSION_LEVEL)
        compressed = cctx.compress(safetensors_bytes)

        # Write compressed bytes
        path.write_bytes(compressed)
```

**2. `pyproject.toml`** (Add dependency)

```toml
[project]
dependencies = [
    "zstandard>=0.22.0",  # zstd compression
    # ... existing dependencies ...
]
```

### Implementation Strategy

1. **Add zstandard dependency** (pyproject.toml)
   - Install: `uv add zstandard`
   - Version: >=0.22.0 (stable API)

2. **Update EmbeddingCache.set()** (embedding_cache.py)
   - Serialize to safetensors (in-memory bytes)
   - Compress with zstd level 3
   - Write to `.safetensors.zst` file

3. **Update EmbeddingCache.get()** (embedding_cache.py)
   - Read compressed file
   - Decompress with zstd
   - Load safetensors from bytes
   - Reconstruct Embedding objects

4. **Remove backward compatibility** (no migration needed)
   - No users exist yet (pre-1.0)
   - Old `.safetensors` files can be deleted manually
   - Add note in CHANGELOG

5. **Unit test compression ratio**
   - Generate synthetic embeddings (1000+ vectors)
   - Verify compressed size is 90-92% of uncompressed (8-10% reduction)
   - Verify roundtrip correctness (save â†’ load â†’ verify)

### Compression Performance

**Benchmarking:**
- zstd level 3: ~500 MB/s compression, ~1500 MB/s decompression
- Typical embedding file: ~100 KB safetensors â†’ ~90-92 KB compressed (8-10% reduction)
- Compression time: <10ms per file
- Decompression time: <5ms per file

**Compression Ratio:**
- Embedding vectors: float32 arrays with high entropy
- Expected reduction: 8-10% size reduction (tested with actual embeddings)
- Compressed size: 90-92% of original safetensors file

**Trade-offs:**
- Level 3: Balance of speed and ratio (zstd recommendation)
- Higher levels (5-10): Better compression, slower (diminishing returns)
- Lower levels (1-2): Faster, worse compression

## Pattern Reuse

### Existing Patterns:

1. **EmbeddingCache API**
   - Pattern: `get(blob_sha)` / `set(blob_sha, embeddings)`
   - Reuse: Preserve API, add compression internally
   - No changes: Callers unaware of compression

2. **Safetensors Format**
   - Pattern: Secure binary format for tensors (existing)
   - Reuse: Compress safetensors bytes (not JSON or pickle)
   - Security: Maintains safetensors safety guarantees

3. **Error Handling**
   - Pattern: Log warnings on cache failures, return None (existing)
   - Reuse: Handle decompression errors gracefully
   - Robustness: Corrupted cache doesn't break indexing

### New Patterns Established:

1. **Transparent Compression**
   - Pattern: Compress on write, decompress on read, API unchanged
   - Reusable: Future storage compression (LanceDB backups, etc.)
   - Standard: zstd is industry standard (used by Facebook, Linux kernel)

2. **Compression Level Configuration**
   - Pattern: Class constant for compression level (level 3)
   - Reusable: Future tuning if performance issues arise
   - Documented: Comment explains tradeoff rationale

## Dependencies

### Prerequisites:

**None** - Story 4 is independent
- Compression is localized to EmbeddingCache
- No dependencies on search quality stories (1-3)
- No dependencies on TUI improvements (Story 5)

### Blocks:

**None** - Story 4 is a leaf node

## Risks & Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| zstandard dependency adds installation complexity | Low | Low | Pure C extension with binary wheels (no build needed) |
| Compression slower than expected (>10ms per 100KB file) | Low | Medium | Benchmark on 1K+ file repos; if >10ms, reduce level from 3 to 1 (modest size reduction tradeoff) |
| Decompression adds search latency | Low | Low | Cache is read during indexing (not search), search uses LanceDB |
| Compressed cache corrupts more easily | Low | Medium | zstd has built-in checksums, safetensors validates format |
| Breaking existing caches | Low | Low | No users yet, no migration needed |

## Tasks

| ID | Title | Status | Hours | Progress |
|----|-------|--------|-------|----------|
| [TASK-0001.4.4.1](TASK-0001.4.4.1.md) | Write BDD Scenarios for Compression Transparency | âœ… Complete | 2 | 2h |
| [TASK-0001.4.4.2](TASK-0001.4.4.2.md) | Add zstandard Dependency and Compression Constants | âœ… Complete | 2 | 2h |
| [TASK-0001.4.4.3](TASK-0001.4.4.3.md) | Implement Compression in set() and Decompression in get() | âœ… Complete | 3 | 3h |
| [TASK-0001.4.4.4](TASK-0001.4.4.4.md) | Verify Compression Ratio and Performance Benchmarks | ðŸ”µ Not Started | 1 | - |

**Total Hours**: 8 (matches 2 story points Ã— 4h/point)

**BDD Progress**: 0/2 scenarios passing

**Incremental BDD Tracking:**
- TASK-1: 0/2 (all scenarios stubbed, failing) âœ…
- TASK-2: 0/2 (foundation only) âœ…
- TASK-3: 2/2 (complete âœ…)
- TASK-4: 2/2 (verification âœ…)

---

**Created**: 2025-10-16
**Last Updated**: 2025-10-16
