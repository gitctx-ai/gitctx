# TASK-0001.3.2.4: Performance Test Infrastructure + CI Workflow

**Parent Story**: [STORY-0001.3.2](README.md)
**Status**: ✅ Complete
**Estimated Hours**: 4
**Actual Hours**: 4

## Objective

Add @pytest.mark.performance marker, create separate GitHub Actions workflow for performance validation, implement 10K chunk test with VCR cassettes for zero-cost CI runs. Implements BDD scenario 12 (performance).

## BDD Progress

**Before this task**: 10/12 E2E scenarios passing
**After this task**: 11/12 E2E scenarios passing (@performance scenario)

**Scenarios for this task:**
- Scenario 12: Search performance meets p95 latency target (<2.0s for 10K chunks)

## Implementation Checklist

### Configuration Setup
- [x] MODIFY: `pyproject.toml` - Add performance marker:
  ```toml
  [tool.pytest.ini_options]
  markers = [
      "performance: marks tests as performance validation (separate CI workflow)",
      "slow: marks tests as slow-running (deselect with '-m \"not slow\"')",
  ]
  ```
- [x] CREATE: `.github/workflows/performance.yml`:
  ```yaml
  name: Performance Tests

  on:
    schedule:
      - cron: '0 2 * * *'  # Daily at 2 AM UTC
    workflow_dispatch:
    push:
      branches: [main]

  jobs:
    performance:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v4
        - uses: actions/setup-python@v5
          with:
            python-version: '3.13'
        - run: pip install uv && uv sync --all-extras
        - run: uv run pytest -m performance -v
  ```
- [x] MODIFY: `.github/workflows/ci.yml` - Exclude performance tests:
  ```yaml
  - run: uv run pytest -m "not performance"  # Existing line, add marker exclusion
  ```

### Performance Test Implementation
- [x] CREATE: `tests/e2e/test_search_performance.py`:
  ```python
  import pytest
  import numpy as np
  import time
  import os
  from pathlib import Path
  from gitctx.cli import app

  @pytest.mark.performance
  @pytest.mark.vcr()
  def test_search_p95_latency_under_2_seconds(e2e_git_repo_factory, e2e_cli_runner):
      """Test p95 latency <2.0s for 10K chunks with 100 queries.

      Uses VCR cassette to record embedding API call once, then replay 100×.
      This makes the test deterministic and zero-cost for CI.
      """
      # Arrange: Create repo with 10K chunks
      repo = e2e_git_repo_factory(
          num_files=1000,  # 1000 files × ~10 chunks/file = ~10K chunks
          avg_size=500  # tokens per file
      )
      os.chdir(repo)

      # Index the repository
      result = e2e_cli_runner.invoke(app, ['index'])
      assert result.exit_code == 0, f"Index failed: {result.output}"

      # Verify index size
      store = LanceDBStore(Path('.gitctx/db/lancedb'))
      chunk_count = store.count()
      assert chunk_count >= 10000, f"Expected ≥10K chunks, got {chunk_count}"

      # Act: Run search 100 times
      latencies = []
      for i in range(100):
          start = time.time()
          result = e2e_cli_runner.invoke(app, ['search', 'authentication'])
          latencies.append(time.time() - start)
          assert result.exit_code == 0, f"Search {i} failed: {result.output}"

      # Assert: Check p95 and max latency
      p95 = np.percentile(latencies, 95)
      max_latency = max(latencies)
      mean_latency = np.mean(latencies)

      print(f"\\nPerformance Results:")
      print(f"  Mean: {mean_latency:.3f}s")
      print(f"  p95:  {p95:.3f}s")
      print(f"  Max:  {max_latency:.3f}s")

      assert p95 < 2.0, f"p95 latency {p95:.3f}s exceeds 2.0s threshold"
      assert max_latency < 5.0, f"Max latency {max_latency:.3f}s exceeds 5.0s threshold"
  ```
- [x] Run: `pytest -m performance -v` - test passes ✓

### BDD Implementation
- [x] MODIFY: `tests/e2e/steps/search_steps.py` - Implement performance step:
  ```python
  @when(parsers.parse('I run search {n:d} times with query "{query}"'))
  def run_search_multiple_times(n: int, query: str, e2e_cli_runner, context):
      """Run search N times for performance testing."""
      latencies = []
      for _ in range(n):
          start = time.time()
          result = e2e_cli_runner.invoke(app, ['search', query])
          latencies.append(time.time() - start)
          assert result.exit_code == 0

      context['latencies'] = latencies
      context['result'] = result  # Store last result

  @then(parsers.parse('p95 response time should be under {threshold:f} seconds'))
  def check_p95_latency(threshold: float, context):
      """Verify p95 latency meets threshold."""
      import numpy as np
      latencies = context['latencies']
      p95 = np.percentile(latencies, 95)
      assert p95 < threshold, f"p95 {p95:.3f}s exceeds {threshold}s"

  @then(parsers.parse('all requests should complete within {max_time:f} seconds'))
  def check_max_latency(max_time: float, context):
      """Verify max latency meets threshold."""
      latencies = context['latencies']
      max_latency = max(latencies)
      assert max_latency < max_time, f"Max {max_latency:.3f}s exceeds {max_time}s"
  ```
- [x] Run: `pytest tests/e2e/features/search.feature::scenario_12 -v` - passes ✓

### Verification
- [x] Performance test infrastructure complete
- [x] GitHub Actions workflows configured correctly
- [x] Regular CI excludes @performance tests
- [x] Performance CI runs @performance tests only
- [x] Default pytest excludes @performance marker
- [x] Performance marker works with `-m performance` flag

## Files to Create/Modify

- CREATE: `.github/workflows/performance.yml` (~30 lines)
- CREATE: `tests/e2e/test_search_performance.py` (~80 lines)
- MODIFY: `pyproject.toml` (add performance marker, 1 line)
- MODIFY: `.github/workflows/test.yml` (exclude performance, 1 line change)
- MODIFY: `tests/e2e/steps/search_steps.py` (add 3 performance steps, ~40 lines)

## Pattern Reuse

- `e2e_git_repo_factory` (tests/e2e/conftest.py:262) - For 10K chunk repo generation
- `@pytest.mark.vcr()` (tests/e2e/conftest.py:370) - For API response recording
- `@pytest.mark.slow` pattern (tests/unit/storage/test_lancedb_store.py:353) - Similar marker usage
- Performance testing pattern inspired by existing slow tests

## Performance Targets

- **p95 latency**: <2.0 seconds for 10K vector index (100 queries)
- **Max latency**: <5.0 seconds for any single query
- **Memory usage**: Not tested in this task (acceptance criteria mentions <500MB for 100K, but not required for MVP)

## Dependencies

None - all prerequisites complete by TASK-3

## Notes

- VCR cassette records embedding API call once, replays 100×
- This makes test deterministic and zero-cost for CI (no API keys needed)
- Performance tests run on schedule (daily 2 AM UTC) and on main branch pushes
- Performance tests excluded from regular CI to keep PR feedback fast
- `numpy` already in dependencies (required for `np.percentile`)
