# TASK-0001.3.1.2: Model Registry and Provider Infrastructure

**Parent Story**: [STORY-0001.3.1](./README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 3
**Actual Hours**: _[To be filled on completion]_

## Deliverable

Model registry with specs, provider factory, and OpenAI provider implementation. All unit tested via TDD. First BDD scenario passing with VCR cassette (1/5).

**Rationale**: Creates the `models/` infrastructure layer that both indexing and search will use. Enables multi-provider support in INIT-0004.

## Prerequisites

- [ ] TASK-0001.3.1.0 completed (architecture refactor, models/ module exists)
- [ ] TASK-0001.3.1.1 completed (BDD scenarios defined)

## Implementation Checklist

### Phase 1: Model Registry (TDD - 1 hour)

#### Write Tests First (RED)

- [ ] Create `tests/unit/models/test_registry.py`

- [ ] Test: Get valid model spec:
  ```python
  def test_get_model_spec_text_embedding_3_large():
      spec = get_model_spec("text-embedding-3-large")
      assert spec["max_tokens"] == 8191
      assert spec["dimensions"] == 3072
      assert spec["provider"] == "openai"
  ```

- [ ] Test: Get small model spec:
  ```python
  def test_get_model_spec_text_embedding_3_small():
      spec = get_model_spec("text-embedding-3-small")
      assert spec["max_tokens"] == 8191
      assert spec["dimensions"] == 1536
      assert spec["provider"] == "openai"
  ```

- [ ] Test: Invalid model raises error:
  ```python
  def test_get_model_spec_invalid_model():
      with pytest.raises(ValueError, match="Unsupported model"):
          get_model_spec("nonexistent-model")
  ```

- [ ] Test: Error message lists supported models:
  ```python
  def test_error_message_lists_supported():
      try:
          get_model_spec("invalid")
      except ValueError as e:
          assert "text-embedding-3-large" in str(e)
          assert "text-embedding-3-small" in str(e)
  ```

- [ ] Run tests - verify all fail:
  ```bash
  uv run pytest tests/unit/models/test_registry.py -v
  # Should show 4 failures
  ```

#### Implement to Pass (GREEN)

- [ ] Open `src/gitctx/models/registry.py` (created in TASK-0001.3.1.0)

- [ ] Implement `ModelSpec` TypedDict:
  ```python
  from typing import TypedDict

  class ModelSpec(TypedDict):
      """Metadata for a supported model."""
      max_tokens: int
      dimensions: int
      provider: str
  ```

- [ ] Implement `MODELS` registry:
  ```python
  MODELS: dict[str, ModelSpec] = {
      "text-embedding-3-large": {
          "max_tokens": 8191,
          "dimensions": 3072,
          "provider": "openai",
      },
      "text-embedding-3-small": {
          "max_tokens": 8191,
          "dimensions": 1536,
          "provider": "openai",
      },
  }
  ```

- [ ] Implement `get_model_spec()`:
  ```python
  def get_model_spec(model_name: str) -> ModelSpec:
      """Get model metadata or raise ValueError."""
      if model_name not in MODELS:
          supported = ", ".join(MODELS.keys())
          raise ValueError(
              f"Unsupported model: {model_name}\n"
              f"Supported: {supported}"
          )
      return MODELS[model_name]
  ```

- [ ] Run tests - verify all pass:
  ```bash
  uv run pytest tests/unit/models/test_registry.py -v
  # Should show 4 passed
  ```

### Phase 2: Model Protocols (30 min)

- [ ] Open `src/gitctx/models/protocols.py` (created in TASK-0001.3.1.0)

- [ ] Implement `ModelProvider` protocol:
  ```python
  from typing import Protocol, List
  import numpy as np

  class ModelProvider(Protocol):
      """Protocol for embedding/LLM providers."""

      def embed_query(self, text: str) -> np.ndarray:
          """Generate embedding for single query."""
          ...

      def embed_documents(self, texts: List[str]) -> List[np.ndarray]:
          """Generate embeddings for multiple documents."""
          ...
  ```

- [ ] Add docstrings explaining protocol usage:
  ```python
  """
  Protocol-based provider abstraction for multi-model support.

  All embedding providers must implement this protocol.
  Enables swapping providers without changing caller code.

  Example:
      provider: ModelProvider = get_embedder("text-embedding-3-large")
      vector = provider.embed_query("authentication logic")
  """
  ```

- [ ] Verify mypy accepts protocol:
  ```bash
  uv run mypy src/gitctx/models/protocols.py
  ```

### Phase 3: Base Provider Class (30 min)

- [ ] Create `src/gitctx/models/base.py`

- [ ] Implement abstract base provider:
  ```python
  """Base provider class with common functionality."""

  from gitctx.models.registry import get_model_spec, ModelSpec

  class BaseProvider:
      """Abstract base for all model providers."""

      def __init__(self, model_name: str):
          self.model_name = model_name
          self.spec: ModelSpec = get_model_spec(model_name)

      @property
      def max_tokens(self) -> int:
          """Maximum token limit for this model."""
          return self.spec["max_tokens"]

      @property
      def dimensions(self) -> int:
          """Embedding dimension count."""
          return self.spec["dimensions"]
  ```

- [ ] Add unit test for base provider:
  ```python
  # tests/unit/models/test_base.py
  def test_base_provider_loads_spec():
      provider = BaseProvider("text-embedding-3-large")
      assert provider.max_tokens == 8191
      assert provider.dimensions == 3072
  ```

### Phase 4: OpenAI Provider (TDD - 1 hour)

#### Write Tests First (RED)

- [ ] Create `tests/unit/models/providers/test_openai.py`

- [ ] Test: Provider initialization:
  ```python
  def test_openai_provider_init():
      provider = OpenAIProvider("text-embedding-3-large", "fake-key")
      assert provider.model_name == "text-embedding-3-large"
      assert provider.dimensions == 3072
  ```

- [ ] Test: Embed query returns correct shape:
  ```python
  def test_embed_query_shape(mocker):
      mock_client = mocker.patch('langchain_openai.OpenAIEmbeddings')
      mock_client.return_value.embed_query.return_value = [0.1] * 3072

      provider = OpenAIProvider("text-embedding-3-large", "fake-key")
      result = provider.embed_query("test")

      assert isinstance(result, np.ndarray)
      assert result.shape == (3072,)
  ```

- [ ] Test: Embed documents returns list of arrays:
  ```python
  def test_embed_documents_shape(mocker):
      mock_client = mocker.patch('langchain_openai.OpenAIEmbeddings')
      mock_client.return_value.embed_documents.return_value = [
          [0.1] * 3072,
          [0.2] * 3072,
      ]

      provider = OpenAIProvider("text-embedding-3-large", "fake-key")
      results = provider.embed_documents(["test1", "test2"])

      assert len(results) == 2
      assert all(isinstance(r, np.ndarray) for r in results)
      assert all(r.shape == (3072,) for r in results)
  ```

- [ ] Run tests - verify all fail:
  ```bash
  uv run pytest tests/unit/models/providers/test_openai.py -v
  ```

#### Implement OpenAI Provider (GREEN)

- [ ] Create `src/gitctx/models/providers/openai.py`

- [ ] Implement OpenAI provider:
  ```python
  """OpenAI embedding provider."""

  import numpy as np
  from typing import List
  from langchain_openai import OpenAIEmbeddings
  from pydantic import SecretStr

  from gitctx.models.base import BaseProvider

  class OpenAIProvider(BaseProvider):
      """OpenAI embedding provider using LangChain."""

      def __init__(self, model_name: str, api_key: str):
          super().__init__(model_name)
          self._client = OpenAIEmbeddings(
              model=model_name,
              api_key=SecretStr(api_key),
              dimensions=self.dimensions,
          )

      def embed_query(self, text: str) -> np.ndarray:
          """Generate embedding for single query."""
          return np.array(self._client.embed_query(text))

      def embed_documents(self, texts: List[str]) -> List[np.ndarray]:
          """Generate embeddings for multiple documents."""
          embeddings = self._client.embed_documents(texts)
          return [np.array(emb) for emb in embeddings]
  ```

- [ ] Run tests - verify all pass:
  ```bash
  uv run pytest tests/unit/models/providers/test_openai.py -v
  ```

### Phase 5: Provider Factory (TDD - 30 min)

#### Write Tests First (RED)

- [ ] Create `tests/unit/models/test_factory.py`

- [ ] Test: Get OpenAI embedder:
  ```python
  def test_get_embedder_openai(mocker):
      mock_settings = mocker.Mock()
      mock_settings.get.return_value = "fake-api-key"

      embedder = get_embedder("text-embedding-3-large", mock_settings)

      assert isinstance(embedder, OpenAIProvider)
      assert embedder.model_name == "text-embedding-3-large"
  ```

- [ ] Test: Missing API key raises error:
  ```python
  def test_get_embedder_missing_key(mocker):
      mock_settings = mocker.Mock()
      mock_settings.get.return_value = None

      with pytest.raises(ConfigurationError, match="API key not configured"):
          get_embedder("text-embedding-3-large", mock_settings)
  ```

- [ ] Test: Invalid model raises error:
  ```python
  def test_get_embedder_invalid_model(mocker):
      mock_settings = mocker.Mock()

      with pytest.raises(ValueError, match="Unsupported model"):
          get_embedder("invalid-model", mock_settings)
  ```

- [ ] Run tests - verify all fail:
  ```bash
  uv run pytest tests/unit/models/test_factory.py -v
  ```

#### Implement Factory (GREEN)

- [ ] Create `src/gitctx/models/factory.py`

- [ ] Implement factory:
  ```python
  """Provider factory for model selection."""

  from gitctx.config.settings import GitCtxSettings
  from gitctx.config.errors import ConfigurationError
  from gitctx.models.registry import get_model_spec
  from gitctx.models.protocols import ModelProvider
  from gitctx.models.providers.openai import OpenAIProvider

  def get_embedder(model_name: str, settings: GitCtxSettings) -> ModelProvider:
      """Get embedding provider for model.

      Args:
          model_name: Model identifier (e.g., "text-embedding-3-large")
          settings: Application settings

      Returns:
          Provider instance implementing ModelProvider protocol

      Raises:
          ValueError: If model not supported
          ConfigurationError: If API key missing
      """
      # Validate model exists
      spec = get_model_spec(model_name)

      # Route to provider
      if spec["provider"] == "openai":
          api_key = settings.get("api_keys.openai")
          if not api_key:
              raise ConfigurationError(
                  "OpenAI API key not configured\n"
                  "Set with: export OPENAI_API_KEY=sk-...\n"
                  "Or run: gitctx config set api_keys.openai sk-..."
              )
          return OpenAIProvider(model_name, api_key)

      raise ValueError(f"Unknown provider: {spec['provider']}")
  ```

- [ ] Run tests - verify all pass:
  ```bash
  uv run pytest tests/unit/models/test_factory.py -v
  ```

### Phase 6: Model Errors (15 min)

- [ ] Open `src/gitctx/models/errors.py` (may exist from TASK-0001.3.1.0)

- [ ] Implement model-specific errors:
  ```python
  """Model-specific errors."""

  from gitctx.exceptions import GitCtxError

  class ModelError(GitCtxError):
      """Base error for model operations."""

  class RateLimitError(ModelError):
      """API rate limit exceeded."""

  class APIError(ModelError):
      """API request failed."""

  class ModelNotFoundError(ModelError):
      """Model not found in registry."""
  ```

### Phase 7: First BDD Scenario (30 min)

- [ ] Verify VCR cassettes directory exists:
  ```bash
  mkdir -p tests/e2e/cassettes/
  ```

- [ ] Set OPENAI_API_KEY for VCR recording:
  ```bash
  export OPENAI_API_KEY=sk-...  # Real key for initial recording
  ```

- [ ] Run first BDD scenario to record cassette:
  ```bash
  uv run pytest tests/e2e/features/search.feature::test_search_scenarios[query-embedding-generated-successfully] -v
  ```

- [ ] Implement minimal steps needed for first scenario:
  - [ ] `indexed_repository` step (run gitctx index)
  - [ ] `env_var_from_real` step (pass through $OPENAI_API_KEY)
  - [ ] `run_search_command` step (run gitctx search - will fail without search implementation, that's OK)

- [ ] Note: First scenario may partially pass (embedding generated) but search not yet implemented. That's expected.

- [ ] Verify VCR cassette created:
  ```bash
  ls tests/e2e/cassettes/
  # Should contain cassette file
  ```

### Phase 8: Final Verification (15 min)

- [ ] Run all model unit tests:
  ```bash
  uv run pytest tests/unit/models/ -v --cov=src/gitctx/models --cov-report=term-missing
  ```

- [ ] Verify coverage >90%:
  - registry.py: 100%
  - protocols.py: N/A (protocol definition)
  - base.py: 100%
  - factory.py: 100%
  - providers/openai.py: >90%

- [ ] Run quality gates:
  ```bash
  uv run ruff check src tests
  uv run mypy src
  ```

- [ ] Verify no circular imports:
  ```bash
  uv run python -c "from gitctx.models.factory import get_embedder; print('âœ“')"
  ```

- [ ] Test imports work correctly:
  ```bash
  uv run python -c "from gitctx.models.registry import get_model_spec; print(get_model_spec('text-embedding-3-large'))"
  ```

## Success Criteria

- [ ] Model registry implemented with 2 model specs
- [ ] ModelProvider protocol defined
- [ ] BaseProvider abstract class created
- [ ] OpenAIProvider implements protocol
- [ ] Provider factory routes to correct provider
- [ ] All unit tests pass (>90% coverage)
- [ ] First BDD scenario passes or partially passes with VCR
- [ ] Quality gates pass (ruff, mypy)
- [ ] No circular import dependencies

## Pattern Reuse

- **Protocol Pattern**: Same as `storage/protocols.py` (VectorStore)
- **Factory Pattern**: Model selection based on configuration
- **LangChain Integration**: Wraps `OpenAIEmbeddings` from EPIC-0001.2
- **Error Hierarchy**: Inherits from `gitctx.exceptions.GitCtxError`
- **TDD Workflow**: RED â†’ GREEN â†’ REFACTOR

## BDD Progress

**Before**: 0/5 scenarios passing
**After**: 1/5 scenarios passing (first scenario with VCR cassette)

Note: First scenario may be partially passing (embedding generated) but full search not yet implemented.

## Files Changed

**Created**:
- `src/gitctx/models/base.py` (~30 lines)
- `src/gitctx/models/factory.py` (~50 lines)
- `src/gitctx/models/providers/openai.py` (~80 lines)
- `tests/unit/models/test_registry.py` (~60 lines)
- `tests/unit/models/test_base.py` (~20 lines)
- `tests/unit/models/test_factory.py` (~50 lines)
- `tests/unit/models/providers/test_openai.py` (~70 lines)

**Modified**:
- `src/gitctx/models/registry.py` (~60 lines implement)
- `src/gitctx/models/protocols.py` (~40 lines implement)
- `src/gitctx/models/errors.py` (~40 lines implement)

**Total**: 10 files, ~500 lines

## Commit Message

```
feat(TASK-0001.3.1.2): Implement model registry and provider infrastructure

- Create model registry with text-embedding-3-large/small specs
- Define ModelProvider protocol for multi-provider support
- Implement BaseProvider abstract class with common functionality
- Create OpenAIProvider wrapping LangChain OpenAIEmbeddings
- Implement provider factory with API key validation
- Add model-specific error hierarchy

All unit tested via TDD (>90% coverage).
First BDD scenario passing with VCR cassette recording.

BDD Progress: 1/5 scenarios passing

ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

---

**Created**: 2025-10-12
**Status**: Ready for implementation
