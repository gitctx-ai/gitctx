# TASK-0001.2.2.3: Implement LanguageAwareChunker with unit tests (TDD) and BDD scenarios

**Parent**: [STORY-0001.2.2](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 10
**Actual Hours**: _____

## Implementation Checklist

### Unit Tests First (TDD - RED)

- [ ] Write unit tests in `tests/unit/core/test_chunker.py`
  - [ ] Test initialization with default overlap ratio
  - [ ] Test initialization with custom overlap ratio
  - [ ] Test `_create_splitter` for known language (python)
  - [ ] Test `_create_splitter` for unknown language (fallback to generic)
  - [ ] Test `_create_splitter` handles ValueError from unsupported LangChain language
  - [ ] Test `count_tokens` matches tiktoken directly
  - [ ] Test `chunk_file` with content < max_tokens â†’ single chunk
  - [ ] Test `chunk_file` with content > max_tokens â†’ multiple chunks
  - [ ] Test `chunk_file` with empty string â†’ empty list
  - [ ] Test `chunk_file` with only whitespace
  - [ ] Test overlap calculation (consecutive chunks share content)
  - [ ] Test line number progression (approximate with overlap)
  - [ ] Test metadata fields present and correct
  - [ ] Test factory function `create_chunker()`
- [ ] Write edge case tests in `tests/unit/core/test_chunker_edge_cases.py`
  - [ ] Single character content
  - [ ] Content with only newlines
  - [ ] Content with very long lines (> max_tokens per line)
  - [ ] Content with mixed line endings (\n, \r\n, \r)
  - [ ] Content at exact max_tokens boundary
  - [ ] Zero overlap ratio (overlap_ratio=0.0)
  - [ ] Maximum overlap ratio (overlap_ratio=0.5)
  - [ ] Content with null bytes (\x00)
  - [ ] Content with control characters
- [ ] Write token ratio validation tests in `tests/unit/core/test_token_ratio.py`
  - [ ] Generate 100 random code samples across different languages
  - [ ] For each sample: chunk with max_tokens=800
  - [ ] Verify every chunk has token_count â‰¤ 880 (max_tokens * 1.1)
  - [ ] Record actual chars-per-token ratio
  - [ ] Verify 3.5 chars-per-token assumption is conservative
  - [ ] Document actual observed ratio range
  - [ ] Fail loudly if any chunk exceeds limit
- [ ] Run tests - all should fail (RED phase) âœ“

### Implementation (GREEN)

- [ ] Create `LanguageAwareChunker` class in `src/gitctx/core/chunker.py`
  - [ ] Implement `ChunkerProtocol` interface
  - [ ] Initialize tiktoken encoder (cl100k_base)
  - [ ] Store `chunk_overlap_ratio: float` (default 0.2)
  - [ ] Define `language_map: dict[str, str]` mapping to LangChain language codes
- [ ] Implement `_create_splitter(language: str, max_tokens: int)` method
  - [ ] Calculate `chunk_size = int(max_tokens * 3.5)` (chars-per-token approximation)
  - [ ] Calculate `chunk_overlap = int(chunk_size * chunk_overlap_ratio)`
  - [ ] Get langchain_lang from language_map
  - [ ] Try `RecursiveCharacterTextSplitter.from_language()` if language known
  - [ ] Fall back to generic `RecursiveCharacterTextSplitter()` if unsupported
  - [ ] Handle ValueError from unsupported languages gracefully
  - [ ] **No caching** (CLI = new process per run)
- [ ] Implement `chunk_file(content: str, language: str, max_tokens: int)` method
  - [ ] Return empty list for empty content
  - [ ] Create splitter via `_create_splitter()`
  - [ ] Split content via `splitter.split_text(content)`
  - [ ] Track line numbers: `current_line` starts at 1
  - [ ] For each chunk:
    - [ ] Count lines: `lines_in_chunk = text.count("\n") + 1`
    - [ ] Create CodeChunk with content, start_line, end_line, token_count
    - [ ] Add metadata: chunk_index, language, max_tokens, overlap_ratio
    - [ ] Update current_line (approximate): `current_line += int(lines_in_chunk * (1 - overlap_ratio))`
  - [ ] Return list of CodeChunk objects
- [ ] Implement `count_tokens(text: str)` method
  - [ ] Use tiktoken encoder to encode text
  - [ ] Return token count: `len(self.encoder.encode(text))`
- [ ] Create factory function `create_chunker(chunk_overlap_ratio: float = 0.2)`
  - [ ] Returns LanguageAwareChunker instance
- [ ] Handle invalid UTF-8 gracefully
  - [ ] Catch UnicodeDecodeError in chunk_file if needed
  - [ ] Return empty list and log warning
- [ ] Run tests - all should pass (GREEN phase) âœ“
- [ ] Refactor if needed (REFACTOR phase)
- [ ] Verify >90% code coverage for chunker.py

### BDD Implementation (Core Chunking Scenarios)

- [ ] Implement BDD steps in `tests/e2e/steps/test_chunking.py`
  - [ ] Create test data generators (deterministic, seeded random)
    - [ ] Generate Python code with N tokens
    - [ ] Generate code with functions/classes
    - [ ] Generate long single lines
    - [ ] Generate Unicode/emoji content
  - [ ] Scenario 1: "Chunk large Python file with overlap"
    - [ ] @given: Create 5000-token Python blob
    - [ ] @when: Chunk with max_tokens=800, overlap=0.2
    - [ ] @then: Verify 7-8 chunks, each â‰¤800 tokens, ~20% overlap, line metadata
  - [ ] Scenario 2: "Small blob returns single chunk"
    - [ ] @given: Create 500-token Python blob
    - [ ] @when: Chunk with max_tokens=800
    - [ ] @then: Verify exactly 1 chunk, full content, start_line=1
  - [ ] Scenario 3: "Language-aware splitting preserves function boundaries"
    - [ ] @given: Python blob with 3 functions (1200 tokens)
    - [ ] @when: Chunk with max_tokens=800
    - [ ] @then: Verify functions not split mid-body (best effort)
  - [ ] Scenario 4: "Long single line handling"
    - [ ] @given: Blob with one 2000-token line
    - [ ] @when: Chunk with max_tokens=800
    - [ ] @then: Verify 3 chunks, no data loss
  - [ ] Scenario 5: "Unicode and emoji support"
    - [ ] @given: Blob with Unicode/emoji
    - [ ] @when: Count tokens and chunk
    - [ ] @then: Verify tiktoken handles non-ASCII, no corruption
  - [ ] Scenario 6: "Multiple language support" (complete implementation)
    - [ ] @given: Blobs in Python, JS, TS, Go, Rust, Java, Markdown
    - [ ] @when: Chunk each
    - [ ] @then: Verify language-specific splitting, language in metadata
  - [ ] Scenario 9: "Token limit compliance verification"
    - [ ] @given: 100 random blobs of varying sizes
    - [ ] @when: Chunk all with max_tokens=800
    - [ ] @then: Verify 100% chunks â‰¤880 tokens (10% tolerance)
- [ ] Run BDD scenarios - Scenarios 1-6, 8-9 should pass âœ“

### Verification

- [ ] All unit tests pass
- [ ] All edge case tests pass
- [ ] Token ratio validation passes (3.5 is conservative)
- [ ] Code coverage >90% for chunker.py
- [ ] BDD Scenarios 1-6, 8-9 pass (7 requires integration)
- [ ] Documentation updated

## Pattern Reuse

**TDD Workflow:**

1. Write failing test (RED)
2. Write minimal code to pass (GREEN)
3. Refactor if needed (REFACTOR)
4. Repeat

**Existing Test Patterns:**

- `tests/unit/core/test_commit_walker.py` - Follow AAA pattern
- Parametrized tests for multiple languages

**LangChain Integration:**

- Import: `from langchain_text_splitters import RecursiveCharacterTextSplitter`
- Import: `import tiktoken`

## Test Requirements

**Coverage Target:**
>
- >90% line coverage for chunker.py
- >85% branch coverage for chunker.py
- 100% of unit tests passing

**BDD Scenarios Passing (7/9):**

- âœ“ Scenario 1: Large file with overlap
- âœ“ Scenario 2: Small blob
- âœ“ Scenario 3: Function boundaries
- âœ“ Scenario 4: Long lines
- âœ“ Scenario 5: Unicode/emoji
- âœ“ Scenario 6: Multiple languages
- Scenario 7: Metadata completeness (partial - needs integration)
- âœ“ Scenario 8: Empty content
- âœ“ Scenario 9: Token limits

## Verification Criteria

- LanguageAwareChunker implements ChunkerProtocol
- Chunks never exceed max_tokens (with 10% tolerance)
- Consecutive chunks have ~20% overlap (configurable)
- Line numbers are tracked (approximate for overlap > 0)
- Token counting matches tiktoken exactly
- Small files return single chunk
- Empty content returns empty list
- Unsupported languages fall back gracefully
- Factory function works
- >90% code coverage achieved
- Most BDD scenarios passing

## Files to Create/Modify

- `tests/unit/core/test_chunker.py` (new)
- `tests/unit/core/test_chunker_edge_cases.py` (new)
- `tests/unit/core/test_token_ratio.py` (new)
- `tests/unit/conftest.py` (modify - add chunker fixtures)
- `src/gitctx/core/chunker.py` (new)
- `tests/e2e/steps/test_chunking.py` (modify - implement core chunking steps)
- `tests/e2e/conftest.py` (modify - add test data generators)

## Dependencies

- TASK-0001.2.2.1 (BDD scenarios written)
- TASK-0001.2.2.2 (protocols, models, language detection)
- `langchain-text-splitters>=0.3.0,<1.0` (add to pyproject.toml)
- `tiktoken>=0.8.0,<1.0` (add to pyproject.toml)

## Notes

- **TDD: Tests first, then implementation**
- Follow RED â†’ GREEN â†’ REFACTOR cycle
- **BDD: Implement core chunking scenarios as you build**
- **No caching of splitters** in MVP
- Token ratio validation is critical - must verify 3.5 assumption
- Line numbers are approximate when overlap > 0
- Generate deterministic test data (seeded random) for reproducibility
- This is the core implementation task - most complex in the story (10 hours, 40+ unit tests, 7 BDD scenarios)
- By end of this task, 7/9 BDD scenarios should pass

---

**Created**: 2025-10-07
**Last Updated**: 2025-10-07
