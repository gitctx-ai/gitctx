# TASK-0001.2.2.3: Implement LanguageAwareChunker with unit tests (TDD) and BDD scenarios

**Parent**: [STORY-0001.2.2](README.md)
**Status**: ✅ Complete
**Estimated Hours**: 10
**Actual Hours**: 10

## Implementation Checklist

### Phase 0: Dependencies (Completed ✅)

- [x] Add `langchain-text-splitters>=0.3.0,<1.0` to pyproject.toml dependencies
- [x] Add `tiktoken>=0.8.0,<1.0` to pyproject.toml dependencies
- [x] Add mypy overrides for langchain_text_splitters and tiktoken
- [x] Run `uv sync --all-extras` to install dependencies (preserves dev tools)
- [x] Extend `language_detection.py` with comprehensive LangChain mapping
  - [x] Expand EXTENSION_TO_LANGUAGE to 60+ file extensions
  - [x] Add all 27 LangChain-supported languages
  - [x] Create LANGUAGE_TO_LANGCHAIN mapping dict
  - [x] Implement `get_langchain_language()` function (testable/replaceable)
  - [x] Add source reference: https://github.com/langchain-ai/langchain/blob/master/libs/text-splitters/langchain_text_splitters/base.py
  - [x] Update docstrings to reflect 95%+ coverage
- [x] Add comprehensive tests for language mapping (83 tests, all passing)
  - [x] Test all 27 LangChain languages in parametrized test
  - [x] Test `get_langchain_language()` for all supported languages
  - [x] Test case-insensitive mapping
  - [x] Test unknown language fallback (returns None)
  - [x] Verify LANGUAGE_TO_LANGCHAIN contains exactly 27 languages
  - [x] Verify all extension languages have LangChain mapping
- [x] Quality gates: ruff check ✅, ruff format ✅, mypy ✅, pytest ✅ (83/83 passed)

### Unit Tests First (TDD - RED)

- [x] Write unit tests in `tests/unit/core/test_chunker.py` (consolidated file with 22 tests in 6 classes)
  - [x] Test initialization with default overlap ratio
  - [x] Test initialization with custom overlap ratio
  - [x] Test `count_tokens` matches tiktoken directly
  - [x] Test token counting with empty string
  - [x] Test `chunk_file` with content < max_tokens → single chunk
  - [x] Test `chunk_file` with content > max_tokens → multiple chunks
  - [x] Test `chunk_file` with empty string → empty list
  - [x] Test `chunk_file` with only whitespace
  - [x] Single character content
  - [x] Content with only newlines
  - [x] Content with very long lines (> max_tokens per line)
  - [x] Content with mixed line endings (\n, \r\n, \r)
  - [x] Content at exact max_tokens boundary
  - [x] Zero overlap ratio (overlap_ratio=0.0)
  - [x] Maximum overlap ratio (overlap_ratio=0.5)
  - [x] Test token ratio validation (100 random samples, all ≤880 tokens with 2.5 chars/token)
  - [x] Test metadata fields present and correct
  - [x] Test factory function `create_chunker()`
  - [x] Test factory function with custom overlap ratio
- [x] Run tests - all should fail (RED phase) ✓

### Implementation (GREEN)

- [x] Create `LanguageAwareChunker` class in `src/gitctx/core/chunker.py`
  - [x] Implement `ChunkerProtocol` interface
  - [x] Initialize tiktoken encoder (cl100k_base)
  - [x] Store `chunk_overlap_ratio: float` (default 0.2)
  - [x] Use `get_langchain_language()` from language_detection module
- [x] Implement `_create_splitter(language: str, max_tokens: int)` method
  - [x] Calculate `chunk_size = int(max_tokens * 2.5)` (conservative chars-per-token)
  - [x] Calculate `chunk_overlap = int(chunk_size * chunk_overlap_ratio)`
  - [x] Get langchain_lang from `get_langchain_language()`
  - [x] Try `RecursiveCharacterTextSplitter.from_language()` if language known
  - [x] Fall back to generic `RecursiveCharacterTextSplitter()` if unsupported
  - [x] Handle ValueError from unsupported languages gracefully
  - [x] **No caching** (CLI = new process per run)
- [x] Implement `chunk_file(content: str, language: str, max_tokens: int)` method
  - [x] Return empty list for empty content
  - [x] Create splitter via `_create_splitter()`
  - [x] Split content via `splitter.split_text(content)`
  - [x] Track line numbers: `current_line` starts at 1
  - [x] For each chunk:
    - [x] Count lines: `lines_in_chunk = text.count("\n") + 1`
    - [x] Create CodeChunk with content, start_line, end_line, token_count
    - [x] Add metadata: chunk_index, language, max_tokens, overlap_ratio
    - [x] Update current_line (approximate): `current_line += int(lines_in_chunk * (1 - overlap_ratio))`
  - [x] Return list of CodeChunk objects
- [x] Implement `count_tokens(text: str)` method
  - [x] Use tiktoken encoder to encode text
  - [x] Return token count: `len(self.encoder.encode(text))`
- [x] Create factory function `create_chunker(chunk_overlap_ratio: float = 0.2)`
  - [x] Returns LanguageAwareChunker instance
- [x] Run tests - all should pass (GREEN phase) ✓
- [x] Refactor (adjusted token ratio from 3.5 to 2.5 for conservative chunking)
- [x] Verify >90% code coverage for chunker.py (95.19% achieved)

### BDD Implementation (Core Chunking Scenarios)

- [x] Implement BDD steps in `tests/e2e/steps/test_chunking.py`
  - [x] Create test data generators (deterministic, seeded random via code_blob_factory fixture)
    - [x] Generate Python code with N tokens
    - [x] Generate code with functions/classes
    - [x] Generate long single lines
    - [x] Generate Unicode/emoji content
  - [x] Scenario 1: "Chunk large Python file with overlap"
    - [x] @given: Create 5000-token Python blob
    - [x] @when: Chunk with max_tokens=800, overlap=0.2
    - [x] @then: Verify 7-8 chunks, each ≤800 tokens, ~20% overlap, line metadata
  - [x] Scenario 2: "Small blob returns single chunk"
    - [x] @given: Create 500-token Python blob
    - [x] @when: Chunk with max_tokens=800
    - [x] @then: Verify exactly 1 chunk, full content, start_line=1
  - [x] Scenario 3: "Language-aware splitting preserves function boundaries"
    - [x] @given: Python blob with 3 functions (1200 tokens)
    - [x] @when: Chunk with max_tokens=800
    - [x] @then: Verify functions not split mid-body (best effort)
  - [x] Scenario 4: "Long single line handling"
    - [x] @given: Blob with one 2000-token line
    - [x] @when: Chunk with max_tokens=800
    - [x] @then: Verify 3 chunks, no data loss
  - [x] Scenario 5: "Unicode and emoji support"
    - [x] @given: Blob with Unicode/emoji
    - [x] @when: Count tokens and chunk
    - [x] @then: Verify tiktoken handles non-ASCII, no corruption
  - [x] Scenario 6: "Multiple language support" (complete implementation)
    - [x] @given: Blobs in Python, JS, TS, Go, Rust, Java, Markdown
    - [x] @when: Chunk each
    - [x] @then: Verify language-specific splitting, language in metadata
  - [x] Scenario 9: "Token limit compliance verification"
    - [x] @given: 100 random blobs of varying sizes
    - [x] @when: Chunk all with max_tokens=800
    - [x] @then: Verify 100% chunks ≤880 tokens (10% tolerance)
- [x] Run BDD scenarios - Scenarios 1-6, 8-9 pass ✓ (8/9 total, Scenario 7 deferred to TASK-4)

### Verification

- [x] All unit tests pass (22/22 passing)
- [x] Token ratio validation passes (2.5 chars/token is conservative, all chunks ≤880 tokens)
- [x] Code coverage >90% for chunker.py (95.19% achieved)
- [x] BDD Scenarios 1-6, 8-9 pass ✓ (8/9 total, Scenario 7 deferred to TASK-4)
- [x] Quality gates: ruff check ✅, ruff format ✅, mypy ✅, pytest ✅

## Pattern Reuse

**TDD Workflow:**

1. Write failing test (RED)
2. Write minimal code to pass (GREEN)
3. Refactor if needed (REFACTOR)
4. Repeat

**Existing Test Patterns:**

- `tests/unit/core/test_commit_walker.py` - Follow AAA pattern
- Parametrized tests for multiple languages

**LangChain Integration:**

- Import: `from langchain_text_splitters import RecursiveCharacterTextSplitter`
- Import: `import tiktoken`

## Test Requirements

**Coverage Target:**
>
- >90% line coverage for chunker.py
- >85% branch coverage for chunker.py
- 100% of unit tests passing

**BDD Scenarios Passing (7/9):**

- ✓ Scenario 1: Large file with overlap
- ✓ Scenario 2: Small blob
- ✓ Scenario 3: Function boundaries
- ✓ Scenario 4: Long lines
- ✓ Scenario 5: Unicode/emoji
- ✓ Scenario 6: Multiple languages
- Scenario 7: Metadata completeness (partial - needs integration)
- ✓ Scenario 8: Empty content
- ✓ Scenario 9: Token limits

## Verification Criteria

- LanguageAwareChunker implements ChunkerProtocol
- Chunks never exceed max_tokens (with 10% tolerance)
- Consecutive chunks have ~20% overlap (configurable)
- Line numbers are tracked (approximate for overlap > 0)
- Token counting matches tiktoken exactly
- Small files return single chunk
- Empty content returns empty list
- Unsupported languages fall back gracefully
- Factory function works
- >90% code coverage achieved
- Most BDD scenarios passing

## Files Created/Modified

- ✅ `tests/unit/core/test_chunker.py` (created - 267 lines, 22 tests in 6 classes)
- ✅ `tests/unit/conftest.py` (modified - added code_blob_factory fixture)
- ✅ `src/gitctx/core/chunker.py` (created - 153 lines, LanguageAwareChunker implementation)
- ✅ `tests/e2e/steps/test_chunking.py` (modified - implemented BDD steps for scenarios 1-6, 8-9)

## Dependencies

- TASK-0001.2.2.1 (BDD scenarios written)
- TASK-0001.2.2.2 (protocols, models, language detection)
- `langchain-text-splitters>=0.3.0,<1.0` (add to pyproject.toml)
- `tiktoken>=0.8.0,<1.0` (add to pyproject.toml)

## Progress Notes

**2025-10-07 - Phase 0 Complete (Dependencies & Language Detection)**

Completed comprehensive language detection foundation:
- Added langchain-text-splitters and tiktoken dependencies to pyproject.toml with mypy overrides
- Extended EXTENSION_TO_LANGUAGE from 20 to 60+ file extensions covering all 27 LangChain languages
- Created LANGUAGE_TO_LANGCHAIN mapping dict for LangChain integration
- Implemented `get_langchain_language()` as separate, testable, replaceable function
- Added 83 comprehensive tests (all passing): 30 new tests for all 27 LangChain languages, case-insensitive mapping, unknown fallback, and bidirectional mapping validation
- Quality gates: ruff check ✅, ruff format ✅, mypy ✅, pytest ✅ (83/83 passed)
- Modified files:
  - `pyproject.toml`: Added langchain-text-splitters, tiktoken, mypy overrides
  - `src/gitctx/core/language_detection.py`: Extended to 148 lines with 60+ extensions, 27 languages, new function
  - `tests/unit/core/test_language_detection.py`: Extended to 250 lines with 83 tests across 2 test classes
- Verified against official LangChain source: https://github.com/langchain-ai/langchain/blob/master/libs/text-splitters/langchain_text_splitters/base.py

**2025-10-07 - TASK Complete ✅**

Implemented LanguageAwareChunker with full TDD/BDD workflow:

**Implementation**:
- Created `src/gitctx/core/chunker.py` (153 lines) with LanguageAwareChunker class
- Implements ChunkerProtocol for future Rust optimization via PyO3
- Uses LangChain's RecursiveCharacterTextSplitter for language-aware chunking
- Uses tiktoken (cl100k_base) for accurate token counting
- Conservative 2.5 chars-per-token ratio ensures chunks never exceed limits

**Unit Tests** (22 tests in 6 classes):
- TestLanguageAwareChunkerInitialization (2 tests)
- TestTokenCounting (2 tests)
- TestChunkFile (3 tests)
- TestChunkerEdgeCases (7 tests with parametrization)
- TestTokenRatioValidation (1 comprehensive test with 100 random samples)
- TestFactoryFunction (2 tests)

**BDD Progress**: 8/9 scenarios passing
- ✅ Scenario 1: Chunk large Python file with overlap
- ✅ Scenario 2: Small blob returns single chunk
- ✅ Scenario 3: Language-aware splitting preserves function boundaries
- ✅ Scenario 4: Long single line handling
- ✅ Scenario 5: Unicode and emoji support
- ✅ Scenario 6: Multiple language support
- ❌ Scenario 7: Chunk metadata completeness (deferred to TASK-4 as planned)
- ✅ Scenario 8: Empty content handling
- ✅ Scenario 9: Token limit compliance verification

**Quality Gates**: All passing
- ruff check ✅
- ruff format ✅
- mypy ✅ (with type ignore for LangChain's restrictive type stubs)
- pytest ✅ (383 tests, 95.19% coverage)

**Key Decisions**:
- Consolidated tests into single `test_chunker.py` with multiple classes (user request)
- Adjusted token ratio from 3.5 to 2.5 after validation showed chunks exceeding limits
- Added `# type: ignore[arg-type]` for LangChain language parameter (library accepts strings at runtime)
- Created `code_blob_factory` fixture in conftest.py for deterministic test data generation

**Files Modified**:
- Created: `src/gitctx/core/chunker.py`
- Created: `tests/unit/core/test_chunker.py`
- Modified: `tests/unit/conftest.py` (added code_blob_factory fixture)
- Modified: `tests/e2e/steps/test_chunking.py` (implemented BDD steps)

## Notes

- **TDD: Tests first, then implementation**
- Follow RED → GREEN → REFACTOR cycle
- **BDD: Implement core chunking scenarios as you build**
- **No caching of splitters** in MVP
- Token ratio validation is critical - must verify 3.5 assumption
- Line numbers are approximate when overlap > 0
- Generate deterministic test data (seeded random) for reproducibility
- This is the core implementation task - most complex in the story (10 hours, 40+ unit tests, 7 BDD scenarios)
- By end of this task, 7/9 BDD scenarios should pass
- **Design improvements (approved 2025-10-07)**:
  - Dependencies: Use `uv sync --all-extras` to preserve dev tools
  - Language mapping: Extracted to `language_detection.get_langchain_language()` as separate, testable, replaceable function
  - Complete LangChain coverage: All 27 supported languages mapped with 60+ file extensions
  - Source reference embedded: https://github.com/langchain-ai/langchain/blob/master/libs/text-splitters/langchain_text_splitters/base.py
  - LangChain supported languages (27): cpp, go, java, kotlin, js, ts, php, proto, python, rst, ruby, rust, scala, swift, markdown, latex, html, sol, csharp, cobol, c, lua, perl, haskell, elixir, powershell, visualbasic6

---

**Created**: 2025-10-07
**Last Updated**: 2025-10-07
