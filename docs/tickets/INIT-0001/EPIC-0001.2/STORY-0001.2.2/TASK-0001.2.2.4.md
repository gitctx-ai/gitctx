# TASK-0001.2.2.4: Integration with CommitWalker, configuration, and final BDD scenario

**Parent**: [STORY-0001.2.2](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 4
**Actual Hours**: _____

## Implementation Checklist

### Integration Tests First

- [ ] Write integration tests in `tests/unit/core/test_chunker_integration.py`
  - [ ] Test walker â†’ language detection â†’ chunker pipeline with real BlobRecord
  - [ ] Test blob_sha injection into chunk metadata
  - [ ] Test UTF-8 decode error handling (skip blob, continue processing)
  - [ ] Test metadata completeness (blob_sha, chunk_index, language, etc.)
  - [ ] Test configuration loading from settings
- [ ] Run integration tests - should fail (implementation doesn't exist yet) ðŸ”´

### Configuration Implementation

- [ ] Extend `GitCtxSettings` with chunking configuration
  - [ ] Add `IndexSettings` nested model in `src/gitctx/config/settings.py`
  - [ ] Field: `max_chunk_tokens: int = 800` with validation (100 â‰¤ x â‰¤ 8191)
  - [ ] Field: `chunk_overlap_ratio: float = 0.2` with validation (0.0 â‰¤ x â‰¤ 0.5)
  - [ ] Add docstrings explaining model-specific token limits
- [ ] Write config tests in `tests/unit/config/test_settings.py`
  - [ ] Test IndexSettings with default values
  - [ ] Test max_chunk_tokens validation (100 â‰¤ x â‰¤ 8191)
  - [ ] Test chunk_overlap_ratio validation (0.0 â‰¤ x â‰¤ 0.5)
  - [ ] Test IndexSettings in GitCtxSettings

### Integration Implementation

- [ ] Document chunking pipeline orchestration in story README
  - [ ] Show: walker â†’ language detection â†’ chunker â†’ chunks with metadata
  - [ ] Verify example code is correct
- [ ] Implement blob_sha injection pattern
  - [ ] After chunking: `chunk.metadata["blob_sha"] = blob_record.sha`
  - [ ] Ensure chunk metadata complete before passing downstream
- [ ] Implement UTF-8 error handling
  - [ ] Wrap `blob_record.content.decode("utf-8")` in try/except
  - [ ] Catch `UnicodeDecodeError` for invalid UTF-8 blobs
  - [ ] Skip blob and log to WalkStats.errors with error_type: "invalid_encoding"
  - [ ] Continue processing remaining blobs
- [ ] Verify integration with existing CommitWalker
  - [ ] Import BlobRecord from models
  - [ ] Use detect_language_from_extension from language_detection
  - [ ] Use create_chunker factory from chunker
  - [ ] Chain components correctly
- [ ] Integration tests pass ðŸŸ¢

### BDD Implementation (Final Scenario)

- [ ] Implement final BDD scenario in `tests/e2e/steps/test_chunking.py`
  - [ ] Scenario 7: "Chunk metadata completeness"
    - [ ] @given: Blob chunked into 5 pieces (with blob_sha injection)
    - [ ] @when: Examine metadata
    - [ ] @then: Verify all fields (content, start_line, end_line, token_count, metadata dict with blob_sha)
- [ ] Run ALL BDD scenarios - all 9 should pass ðŸŸ¢

### Verification

- [ ] All integration tests pass
- [ ] Configuration tests pass
- [ ] UTF-8 errors handled gracefully (no crash)
- [ ] Pipeline chains correctly
- [ ] Metadata is complete (including blob_sha)
- [ ] **ALL 9 BDD scenarios pass** âœ…
- [ ] Documentation updated

## Pattern Reuse

**Existing Patterns:**
- `GitCtxSettings` (src/gitctx/config/settings.py) - Add IndexSettings nested model
- `WalkSettings` nested model - Follow same pattern
- Error handling in walker - Follow same try/except pattern

**Configuration Pattern:**
```python
class IndexSettings(BaseModel):
    """Index configuration."""
    max_chunk_tokens: int = Field(
        default=800,
        ge=100,
        le=8191,
        description="Maximum tokens per chunk (model-specific)"
    )
    chunk_overlap_ratio: float = Field(
        default=0.2,
        ge=0.0,
        le=0.5,
        description="Overlap between chunks (0.0-0.5)"
    )
```

## Test Requirements

**Integration Tests:**
- Walker â†’ chunker pipeline with real BlobRecord
- Language detection from file paths
- blob_sha metadata injection
- UTF-8 decode error handling
- Configuration integration

**Config Tests:**
- Default values
- Validation ranges
- Integration with GitCtxSettings

**BDD Scenarios (9/9 passing):**
- âœ“ Scenario 1: Large file with overlap
- âœ“ Scenario 2: Small blob
- âœ“ Scenario 3: Function boundaries
- âœ“ Scenario 4: Long lines
- âœ“ Scenario 5: Unicode/emoji
- âœ“ Scenario 6: Multiple languages
- âœ“ Scenario 7: Metadata completeness (FINAL - implemented in this task)
- âœ“ Scenario 8: Empty content
- âœ“ Scenario 9: Token limits

## Verification Criteria

- IndexSettings exists in GitCtxSettings
- Configuration validates correctly (ranges enforced)
- Language detection works from BlobRecord.locations
- blob_sha added to every chunk's metadata
- Invalid UTF-8 blobs skipped gracefully
- Pipeline chains correctly
- Chunk metadata complete
- **ALL 9 BDD scenarios pass** (acceptance criteria met) âœ…

## Files to Create/Modify

- `src/gitctx/config/settings.py` (modify - add IndexSettings)
- `tests/unit/config/test_settings.py` (modify - add IndexSettings tests)
- `tests/unit/core/test_chunker_integration.py` (new)
- `tests/e2e/steps/test_chunking.py` (modify - implement Scenario 7)

## Dependencies

- TASK-0001.2.2.1 (BDD scenarios written)
- TASK-0001.2.2.2 (protocols, models, language detection)
- TASK-0001.2.2.3 (chunker implementation)
- STORY-0001.2.1 (CommitWalker and BlobRecord)

## Notes

- This task focuses on wiring existing components together
- No new chunking logic - just integration glue code
- UTF-8 error handling prevents crashes on binary files
- blob_sha injection at integration layer (separation of concerns)
- max_chunk_tokens=800 is conservative for text-embedding-3-large (8191 limit)
- **Final task completes the last BDD scenario**
- **After this task: ALL acceptance criteria verified** âœ…

---

**Created**: 2025-10-07
**Last Updated**: 2025-10-07
