# TASK-0001.2.3.3: Implement OpenAIEmbedder wrapping LangChain (TDD) and BDD scenarios

**Parent**: [STORY-0001.2.3](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 8
**Actual Hours**: -

## Objective

Implement OpenAI embedding generation by wrapping `langchain_openai.OpenAIEmbeddings`. LangChain handles batching, retry logic, and rate limiting automatically. Focus on wrapper logic, cost calculation, and metadata management.

## Implementation Checklist

### Unit Tests First (TDD - RED)
- [ ] Create `tests/unit/embeddings/test_openai_embedder.py`
- [ ] Write 15+ tests for OpenAIEmbedder BEFORE implementation:

**Initialization Tests:**
- [ ] `test_init_with_api_key()` - Valid API key
- [ ] `test_init_without_api_key_raises()` - ConfigurationError
- [ ] `test_init_configures_langchain_embeddings()` - Correct model, dimensions, chunk_size

**Embedding Generation Tests:**
- [ ] `test_embed_single_chunk()` - One chunk â†’ one Embedding
- [ ] `test_embed_multiple_chunks()` - List of chunks â†’ list of Embeddings
- [ ] `test_embed_chunks_batching()` - >2048 chunks handled automatically by LangChain
- [ ] `test_embed_chunks_metadata()` - blob_sha, chunk_index in Embeddings
- [ ] `test_embed_chunks_dimensions()` - Exactly 3072 dimensions
- [ ] `test_embed_chunks_cost_calculation()` - Accurate cost in metadata

**Error Handling Tests:**
- [ ] `test_embed_chunks_rate_limit_retries()` - Mock 429 error, verify LangChain retries
- [ ] `test_embed_chunks_network_error_retries()` - Mock network error, verify retry
- [ ] `test_embed_chunks_max_retries_exceeded()` - Raise after max_retries
- [ ] `test_embed_chunks_invalid_dimensions_raises()` - Dimension mismatch error

**Cost Estimation Tests:**
- [ ] `test_estimate_cost_calculation()` - Formula: token_count * $0.13 / 1M
- [ ] `test_estimate_cost_zero_tokens()` - Edge case: 0 tokens = $0

**Edge Cases (10+ tests):**
- [ ] `test_empty_chunks_list()` - Empty list â†’ empty list
- [ ] `test_single_chunk_edge_case()` - Exactly 1 chunk
- [ ] `test_chunks_exactly_batch_size()` - Exactly 2048 chunks
- [ ] `test_chunks_over_batch_size()` - 2049 chunks (multiple batches)
- [ ] `test_unicode_content()` - Non-ASCII characters
- [ ] `test_very_large_chunk()` - >8191 tokens (OpenAI limit)
- [ ] `test_chunk_with_no_content()` - Empty string content
- [ ] `test_special_characters()` - Code with symbols
- [ ] `test_multiline_content()` - Code with newlines
- [ ] `test_concurrent_embed_calls()` - Multiple async calls

- [ ] Run tests - all fail âœ“ (RED phase)

### Implementation (GREEN)
- [ ] Create `src/gitctx/embeddings/` directory
- [ ] Create `src/gitctx/embeddings/__init__.py`:
  ```python
  from .openai_embedder import OpenAIEmbedder

  __all__ = ["OpenAIEmbedder"]
  ```
- [ ] Create `src/gitctx/embeddings/openai_embedder.py`
- [ ] Implement OpenAIEmbedder class:

**Constants:**
```python
MODEL = "text-embedding-3-large"
DIMENSIONS = 3072
COST_PER_MILLION_TOKENS = 0.13
MAX_BATCH_SIZE = 2048  # OpenAI API limit
```

**Class Structure:**
```python
class OpenAIEmbedder:
    """Wraps langchain_openai.OpenAIEmbeddings for gitctx.

    Provides:
    - Automatic batching (up to 2048 chunks via LangChain)
    - Retry logic with exponential backoff (via LangChain)
    - Rate limiting handling (via LangChain)
    - Cost tracking and metadata
    - Provider abstraction (implements EmbedderProtocol)
    """

    def __init__(
        self,
        api_key: str,
        max_retries: int = 3,
        show_progress: bool = False,
        **kwargs
    ):
        """Initialize OpenAI embedder.

        Args:
            api_key: OpenAI API key
            max_retries: Max retry attempts (default: 3)
            show_progress: Show progress bar for batches
            **kwargs: Additional args for OpenAIEmbeddings
        """
        if not api_key:
            raise ConfigurationError("OpenAI API key is required")

        self._embeddings = OpenAIEmbeddings(
            model=MODEL,
            dimensions=DIMENSIONS,
            chunk_size=MAX_BATCH_SIZE,
            max_retries=max_retries,
            show_progress_bar=show_progress,
            tiktoken_enabled=True,
            check_embedding_ctx_length=True,
            api_key=api_key,
            **kwargs
        )
```

**Methods to Implement:**
- [ ] `async def embed_chunks(self, chunks: list[CodeChunk], blob_sha: str) -> list[Embedding]`
  - [ ] Extract content from chunks: `[chunk.content for chunk in chunks]`
  - [ ] Call LangChain: `vectors = await self._embeddings.aembed_documents(contents)`
  - [ ] Validate dimensions: `assert all(len(v) == DIMENSIONS for v in vectors)`
  - [ ] Build Embedding objects with metadata (blob_sha, chunk_index, token_count, cost)
  - [ ] Return list of Embeddings

- [ ] `def estimate_cost(self, token_count: int) -> float`
  - [ ] Calculate: `token_count * COST_PER_MILLION_TOKENS / 1_000_000`
  - [ ] Return float (USD)

- [ ] Run tests - all pass âœ“ (GREEN phase)

### Refactoring (REFACTOR)
- [ ] Extract helper: `_calculate_cost(token_count: int) -> float`
- [ ] Add logging for batch progress (if `show_progress=True`)
- [ ] Add type hints with mypy validation
- [ ] Add comprehensive docstrings
- [ ] Run tests - still pass âœ“

### BDD Implementation (Core Scenarios)
- [ ] Implement BDD steps for Scenarios 1-6, 8-9:

**Scenario 1: Generate embedding for single chunk**
- [ ] Use `e2e_git_repo_factory` to create test repo
- [ ] Create single chunk
- [ ] Call embedder.embed_chunks()
- [ ] Assert 1 embedding returned, 3072 dimensions

**Scenario 2: Batch process multiple chunks efficiently**
- [ ] Create 5000 chunks (>2048, multiple batches)
- [ ] Call embedder.embed_chunks()
- [ ] Assert all embeddings returned
- [ ] Verify LangChain batching (mock/spy on aembed_documents)

**Scenario 3: Handle rate limit errors with exponential backoff**
- [ ] Mock OpenAIEmbeddings.aembed_documents to raise rate limit error
- [ ] Verify retry happens (LangChain handles this)
- [ ] Assert eventual success

**Scenario 4: Handle network errors gracefully**
- [ ] Mock network error
- [ ] Verify LangChain retries
- [ ] Assert graceful error handling

**Scenario 5: Cache embeddings by blob SHA**
- [ ] Note: Caching implementation in TASK-0001.2.3.4
- [ ] Stub steps for now with NotImplementedError

**Scenario 6: Generate new embeddings for uncached blobs**
- [ ] Note: Caching implementation in TASK-0001.2.3.4
- [ ] Stub steps for now with NotImplementedError

**Scenario 8: Track API costs accurately**
- [ ] Generate embeddings
- [ ] Assert cost calculation: `sum(e.cost_usd for e in embeddings)`
- [ ] Verify formula: `token_count * $0.13 / 1M`

**Scenario 9: Log progress during batch processing**
- [ ] Initialize with `show_progress=True`
- [ ] Generate embeddings
- [ ] Verify progress logged (capture logs or use show_progress_bar)

- [ ] Run BDD tests: `uv run pytest tests/e2e/features/embedding.feature -v`
- [ ] Verify 7/10 scenarios pass (Scenarios 1-4, 8-9, and partial 5-6)

### Verification
- [ ] All unit tests pass: `uv run pytest tests/unit/embeddings/test_openai_embedder.py -v`
- [ ] BDD Progress: 7/10 scenarios passing (Scenarios 5, 6, 7, 10 pending)
- [ ] Code coverage >90%: `uv run pytest --cov=src/gitctx/embeddings --cov-report=term-missing`
- [ ] Type checking: `uv run mypy src/gitctx/embeddings`
- [ ] Quality gates: `uv run ruff check src tests && uv run ruff format src tests`

## Pattern Reuse

**LangChain Patterns:**
- Async embeddings: `await OpenAIEmbeddings.aembed_documents(texts)`
- Batching: Automatic via `chunk_size=2048` parameter
- Retry: Automatic via `max_retries=3` parameter
- Token counting: Automatic via `tiktoken_enabled=True`

**Testing Patterns:**
- Mock LangChain: `pytest-mock` to mock `OpenAIEmbeddings.aembed_documents`
- AAA structure: `tests/unit/core/test_chunker.py:20-95`
- Parametrization: `@pytest.mark.parametrize` for edge cases
- Async tests: `@pytest.mark.anyio` for async methods

**Fixtures Available:**
- `isolated_env` (tests/unit/conftest.py:13) - Environment isolation
- `code_blob_factory` (tests/unit/conftest.py:870) - Generate test chunks
- `e2e_git_repo_factory` (tests/e2e/conftest.py:254) - E2E test repos

## LangChain v1.0 Alpha Benefits

**What LangChain Handles for Us:**
- âœ… Batching: Automatic chunking via `chunk_size` parameter
- âœ… Retry: Exponential backoff (4-20 sec) via `max_retries`
- âœ… Rate Limiting: Built-in 429 error handling
- âœ… Token Counting: Via tiktoken integration
- âœ… Long Input Handling: Auto-split via `check_embedding_ctx_length`
- âœ… Progress Tracking: Via `show_progress_bar`

**What We Implement:**
- âœ… Metadata management (blob_sha, chunk_index)
- âœ… Cost calculation (token_count Ã— $0.13/1M)
- âœ… Domain types (CodeChunk â†’ Embedding)
- âœ… Protocol compliance (EmbedderProtocol)
- âœ… Error translation (LangChain errors â†’ gitctx errors)

**Complexity Reduction:**
- Before (direct OpenAI): ~300 lines (batching, retry, validation)
- After (LangChain wrapper): ~100 lines (focus on domain logic)

## Notes

- LangChain handles the hard parts (batching, retry, rate limiting)
- Focus on wrapper quality and metadata
- Mock `OpenAIEmbeddings.aembed_documents` in unit tests
- Use real API in E2E tests (with test API key in CI)
- Cost tracking is manual (LangChain doesn't expose this)
- Dimension validation critical (3072 exactly)

## Expected Outcome

**BDD Progress**: 7/10 scenarios passing (Scenarios 1-4, 8-9, partial 5-6)
**Result**: Core embedding generation works, ready for integration in TASK-0001.2.3.4

---

**Created**: 2025-10-07
**Last Updated**: 2025-10-07
