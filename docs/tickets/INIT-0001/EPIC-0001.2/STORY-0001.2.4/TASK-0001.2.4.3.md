# TASK-0001.2.4.3: Core Storage Operations & Indexing

**Parent**: [STORY-0001.2.4](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 3
**Actual Hours**: -

## Objective

Implement batch insertion with denormalization, IVF-PQ indexing, vector search, and statistics using TDD + incremental BDD. Complete the core storage operations that enable efficient embedding storage and retrieval with full context.

## Acceptance Criteria

- [ ] `add_chunks_batch()` accepts `Embedding` list + `blob_locations` dict
- [ ] `add_chunks_batch()` denormalizes `BlobLocation` metadata into each chunk record
- [ ] `add_chunks_batch()` logs warning and skips chunks with missing blob locations
- [ ] Batch insertion handles 5000+ chunks with performance >100 chunks/sec (on modern dev laptop: 2020+ model, 8GB+ RAM, SSD)
- [ ] `optimize()` creates IVF-PQ index when `count() >= 256`
- [ ] `optimize()` skips indexing when `count() < 256` (logs info message)
- [ ] IVF-PQ uses cosine metric, adaptive partitions (`min(count // 256, 256)`), adaptive subvectors (`min(3072 // 16, 96)`)
- [ ] `search()` returns results with all 19 denormalized metadata fields
- [ ] `search()` supports `filter_head_only` parameter (only return chunks where `is_head=true`)
- [ ] Schema validation rejects dimension mismatches with error: `"DimensionMismatchError: Dimension mismatch: expected 3072, got {actual}. Action required: Delete .gitctx/lancedb/ and re-index with \`gitctx index --force\`"`
- [ ] `get_statistics()` returns: `total_chunks`, `total_files`, `total_blobs`, `languages` (dict), `index_size_mb`
- [ ] Incremental updates preserve existing chunks (old data unchanged)
- [ ] All unit tests written FIRST (TDD red-green-refactor)
- [ ] Performance test marked with `@pytest.mark.slow`
- [ ] Unit test coverage >90%
- [ ] BDD progress: 7/10 scenarios passing

## Implementation Plan (TDD)

### Step 1: Write add_chunks_batch Tests FIRST (Red) - 30 minutes

```python
# tests/unit/storage/test_lancedb_store.py

def test_add_chunks_batch_denormalizes_blob_location(tmp_path, mock_embeddings, mock_blob_locations):
    """add_chunks_batch denormalizes BlobLocation metadata into each chunk."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    # This test will FAIL until we implement add_chunks_batch
    store.add_chunks_batch(
        embeddings=mock_embeddings,  # 10 embeddings from 2 blobs
        blob_locations=mock_blob_locations
    )

    # Query back and verify denormalized fields
    results = store.chunks_table.to_pandas()
    assert len(results) == 10
    assert "file_path" in results.columns
    assert "commit_sha" in results.columns
    assert results.iloc[0]["author_name"] == "Test Author"

def test_add_chunks_batch_empty_blob_locations_warning(tmp_path, mock_embeddings, caplog):
    """add_chunks_batch logs warning and skips chunks with missing blob locations."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    # Empty blob_locations dict - should warn and skip
    store.add_chunks_batch(embeddings=mock_embeddings, blob_locations={})

    assert "No location found for blob" in caplog.text
    assert store.count() == 0  # No chunks inserted

@pytest.mark.slow
def test_batch_insertion_performance_5000_chunks(tmp_path, mock_large_embedding_batch):
    """Batch insertion handles 5000+ chunks at >100 chunks/sec."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    import time
    start = time.time()
    store.add_chunks_batch(
        embeddings=mock_large_embedding_batch,  # 5000 embeddings
        blob_locations=mock_blob_locations_for_batch
    )
    elapsed = time.time() - start

    # Performance target: >100 chunks/sec (5000 chunks in <50 seconds)
    # Hardware baseline: 2020+ dev laptop (8GB+ RAM, SSD)
    chunks_per_sec = 5000 / elapsed
    assert chunks_per_sec >= 100, f"Too slow: {chunks_per_sec:.1f} chunks/sec (target: 100+)"
```

**Run tests - they FAIL** (red)

### Step 2: Implement add_chunks_batch (Green) - 45 minutes

```python
# src/gitctx/storage/lancedb_store.py

def add_chunks_batch(
    self,
    embeddings: list[Embedding],
    blob_locations: dict[str, list[BlobLocation]]
):
    """Add chunks in batch with denormalized metadata.

    Args:
        embeddings: List of Embedding objects from embedder
        blob_locations: Map of blob_sha -> BlobLocation list (from walker)
    """
    from datetime import UTC, datetime
    import logging

    logger = logging.getLogger(__name__)
    records = []

    for emb in embeddings:
        # Get BlobLocation for this chunk's blob
        locations = blob_locations.get(emb.blob_sha, [])
        if not locations:
            logger.warning(f"No location found for blob {emb.blob_sha[:8]}... - skipping chunk")
            continue

        # Use first location (denormalized schema duplicates this per chunk)
        loc = locations[0]

        record = {
            "vector": emb.vector,
            "chunk_content": emb.chunk_content,
            "token_count": emb.token_count,
            "blob_sha": emb.blob_sha,
            "chunk_index": emb.chunk_index,
            "start_line": emb.start_line,
            "end_line": emb.end_line,
            "total_chunks": emb.total_chunks,
            "file_path": loc.file_path,
            "language": emb.language,
            "commit_sha": loc.commit_sha,
            "author_name": loc.author_name,
            "author_email": loc.author_email,
            "commit_date": loc.commit_date,
            "commit_message": loc.commit_message,
            "is_head": loc.is_head,
            "is_merge": loc.is_merge,
            "embedding_model": emb.model,
            "indexed_at": datetime.now(UTC).isoformat(),
        }
        records.append(record)

    # Batch insert
    if records:
        self.chunks_table.add(records)
        logger.info(f"Inserted {len(records)} chunks into LanceDB")
```

**Run tests - they PASS** (green)

### Step 3: Write optimize Tests FIRST (Red) - 15 minutes

```python
def test_optimize_creates_ivf_pq_index_at_256_vectors(tmp_path, mock_embeddings_256):
    """optimize() creates IVF-PQ index when count >= 256."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_256, mock_blob_locations)

    # This test will FAIL until we implement optimize
    store.optimize()

    # Verify index was created (LanceDB stores index metadata)
    table_stats = store.chunks_table.stats
    assert "index" in table_stats  # Index exists

def test_optimize_skips_indexing_below_256_vectors(tmp_path, caplog):
    """optimize() skips indexing when count < 256."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_100, mock_blob_locations)

    store.optimize()

    assert "Not enough vectors (100) for indexing" in caplog.text
```

**Run tests - they FAIL** (red)

### Step 4: Implement optimize (Green) - 30 minutes

```python
def optimize(self):
    """Create IVF-PQ index for fast vector search.

    Only creates index if we have enough vectors (>=256).
    Index params are adaptive based on row count and dimensions.
    """
    import logging
    logger = logging.getLogger(__name__)

    row_count = self.count()

    if row_count < 256:
        logger.info(f"Not enough vectors ({row_count}) for indexing (minimum: 256)")
        return

    logger.info(f"Creating IVF-PQ index for {row_count} vectors...")

    self.chunks_table.create_index(
        metric="cosine",
        num_partitions=min(row_count // 256, 256),
        num_sub_vectors=min(3072 // 16, 96),  # For 3072-dim embeddings
    )

    logger.info("IVF-PQ vector index created successfully")
```

**Run tests - they PASS** (green)

### Step 5: Write search, statistics Tests FIRST (Red) - 20 minutes

```python
def test_search_returns_denormalized_metadata(tmp_path, mock_embeddings, mock_blob_locations):
    """search() returns results with all 19 denormalized fields."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings, mock_blob_locations)

    query_vector = [0.1] * 3072
    results = store.search(query_vector, limit=5)

    assert len(results) > 0
    first = results[0]
    # Verify all 19 fields present
    assert "vector" in first
    assert "file_path" in first
    assert "commit_sha" in first
    assert "author_name" in first

def test_search_filter_head_only(tmp_path, mock_embeddings_mixed):
    """search() with filter_head_only returns only HEAD chunks."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_mixed, mock_blob_locations)  # 5 HEAD, 5 history

    query_vector = [0.1] * 3072
    results = store.search(query_vector, limit=10, filter_head_only=True)

    # All results should have is_head=True
    assert all(r["is_head"] for r in results)

def test_get_statistics_accuracy(tmp_path, mock_embeddings):
    """get_statistics() returns accurate counts and languages."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings, mock_blob_locations)  # 100 chunks, 10 blobs, 5 files

    stats = store.get_statistics()

    assert stats["total_chunks"] == 100
    assert stats["total_files"] == 5
    assert stats["total_blobs"] == 10
    assert "languages" in stats
    assert isinstance(stats["languages"], dict)
    assert stats["index_size_mb"] > 0
```

**Run tests - they FAIL** (red)

### Step 6: Implement search, get_statistics (Green) - 40 minutes

Complete implementation in `lancedb_store.py`.

**Run tests - they PASS** (green)

### Step 7: Implement BDD Steps for Scenarios 1-4, 6 - 30 minutes

Update `tests/e2e/steps/lancedb_storage_steps.py` to implement:
- Scenario 1: Store embeddings with denormalized metadata
- Scenario 2: Batch insertion for efficiency
- Scenario 3: Automatic IVF-PQ indexing
- Scenario 4: Incremental updates
- Scenario 6: Schema validation

**Run BDD tests**:
```bash
uv run pytest tests/e2e/features/lancedb_storage.feature -v
# Expected: 7/10 scenarios passing (1-4, 6, 9-10)
```

## Unit Tests (Write FIRST - TDD)

### New tests in test_lancedb_store.py (10 tests)

1. `test_add_chunks_batch_denormalizes_blob_location` - Verify 19 fields populated
2. `test_add_chunks_batch_empty_blob_locations_warning` - Warning + skip (use caplog)
3. `test_batch_insertion_performance_5000_chunks` - >100 chunks/sec (@pytest.mark.slow)
4. `test_optimize_creates_ivf_pq_index_at_256_vectors` - Index creation
5. `test_optimize_skips_indexing_below_256_vectors` - Info log + no index
6. `test_search_returns_denormalized_metadata` - All 19 fields in results
7. `test_search_filter_head_only` - Only is_head=True results
8. `test_schema_validation_dimension_mismatch` - DimensionMismatchError with clear message
9. `test_get_statistics_accuracy` - Correct counts, languages dict
10. `test_incremental_updates_preserve_existing_chunks` - Old data unchanged after add

### Edge Cases

**Empty blob_locations**: Log warning with blob SHA (first 8 chars), skip chunk, continue processing

**Dimension mismatch**: Raise `DimensionMismatchError` with exact message:
```
DimensionMismatchError: Dimension mismatch: expected 3072, got 1536.
Action required: Delete .gitctx/lancedb/ and re-index with `gitctx index --force`
```

**Incremental updates**: Existing chunks remain unchanged (test by querying before/after)

## BDD Scenarios (7/10 Passing)

**Scenario 1: Store embeddings with denormalized metadata** âœ…
- Verify 100 embeddings stored with complete BlobLocation data
- Query and confirm all 19 fields present

**Scenario 2: Batch insertion for efficiency** âœ…
- Insert 5000 embeddings
- Verify all inserted successfully
- Verify insertion took reasonable time (<50 seconds on modern hardware)

**Scenario 3: Automatic IVF-PQ indexing** âœ…
- Insert 300 vectors
- Call optimize()
- Verify IVF-PQ index created with correct params

**Scenario 4: Incremental updates** âœ…
- Start with 1000 chunks
- Add 50 new chunks
- Verify 1050 total, old chunks unchanged

**Scenario 6: Schema validation** âœ…
- Existing index: 3072-dim
- Attempt to insert 1024-dim vectors
- Verify DimensionMismatchError with clear message

**Scenarios 9-10** (from TASK-2) âœ…

**Remaining** (TASK-4):
- Scenario 5: Index state tracking
- Scenario 7: Statistics reporting
- Scenario 8: Query with blob location context

## Dependencies

**Requires**:
- TASK-0001.2.4.2 complete (schema, protocol, store foundation)
- Mock fixtures for `Embedding` and `BlobLocation` in `tests/unit/storage/conftest.py`

**Blocks**:
- TASK-0001.2.4.4 (needs core operations complete)

## Performance Targets

**Hardware Baseline**: Modern dev laptop (2020+ model, 8GB+ RAM, SSD)

- **Insertion speed**: >100 chunks/second (5000 chunks in <50 seconds)
- **Search latency**: <500ms for top-10 results (pre-optimization)
- **Index creation**: <5 seconds for 1000 vectors
- **Memory usage**: <500MB for 5000 chunks

**Why these targets**: CLI tool performance must be acceptable on typical developer hardware. Targets are conservative (achievable on older machines) while ensuring good UX.

## Success Criteria

Task complete when:

- âœ… `add_chunks_batch()` implemented with denormalization logic
- âœ… `optimize()` implemented with IVF-PQ index creation
- âœ… `search()` implemented with filter support
- âœ… `get_statistics()` implemented with language breakdown
- âœ… All 10 new unit tests pass (TDD workflow followed)
- âœ… Performance test passes on modern hardware
- âœ… Edge cases handled (empty locations, dimension mismatch)
- âœ… Unit test coverage >90%
- âœ… BDD progress: 7/10 scenarios passing
- âœ… Quality gates pass: ruff, mypy, pytest

## Notes

**TDD Discipline**: Write failing tests first (red), implement minimal code to pass (green), refactor if needed. This ensures all code is tested and requirements are clear before implementation.

**Performance Testing**: Mark with `@pytest.mark.slow` to skip in fast test runs. Document hardware baseline to avoid flaky tests on different machines.

**Denormalization Logic**: Each chunk gets full BlobLocation metadata from first location in list. This duplicates data but eliminates joins (100x faster queries, 3.4% storage overhead per ADR).

---

**Created**: 2025-10-08
**Last Updated**: 2025-10-08
