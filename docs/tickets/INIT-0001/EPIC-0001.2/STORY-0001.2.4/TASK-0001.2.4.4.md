# TASK-0001.2.4.4: Index State Tracking & Final Integration

**Parent**: [STORY-0001.2.4](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 3
**Actual Hours**: -

## Objective

Add index metadata tracking, complete final BDD scenarios, and verify full pipeline integration (walker â†’ chunker â†’ embedder â†’ store). Ensure statistics reporting includes language breakdown and that queries return complete BlobLocation context without joins.

## Acceptance Criteria

- [ ] `save_index_state()` stores: `last_commit`, `indexed_blobs` (JSON list), `last_indexed` (ISO timestamp), `embedding_model`, `total_chunks`, `total_blobs`
- [ ] Metadata table uses upsert pattern: delete old state with `key='index_state'`, insert new state
- [ ] Metadata table operations handle empty table gracefully (no exception on first save)
- [ ] Query results include complete BlobLocation context: `blob_sha`, `file_path`, `start_line`, `end_line`, `commit_sha`, `author_name`, `author_email`, `commit_date`, `commit_message`, `is_head`, `is_merge`
- [ ] Statistics reporting includes `languages` dict with language â†’ count mapping
- [ ] Full E2E pipeline test passes: real walker output â†’ chunker â†’ embedder â†’ store â†’ query
- [ ] Integration test uses actual `BlobRecord` from walker (not mocks)
- [ ] All 10 BDD scenarios pass
- [ ] Code coverage >90% for entire storage module
- [ ] Performance targets verified: >300 chunks/sec insertion, <100ms search (with IVF-PQ index)
- [ ] All unit tests written FIRST (TDD red-green-refactor)

## Implementation Plan (TDD)

### Step 1: Write save_index_state Tests FIRST (Red) - 20 minutes

```python
# tests/unit/storage/test_lancedb_store.py

def test_save_index_state_stores_metadata(tmp_path, mock_blob_locations):
    """save_index_state() stores complete metadata."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_100, mock_blob_locations)

    # This test will FAIL until we implement save_index_state
    store.save_index_state(
        last_commit="abc123def456",
        indexed_blobs=["blob1", "blob2", "blob3"],
        embedding_model="text-embedding-3-large"
    )

    # Query metadata table
    metadata_df = store.metadata_table.to_pandas()
    state = metadata_df[metadata_df["key"] == "index_state"].iloc[0]

    assert state["last_commit"] == "abc123def456"
    assert "blob1" in state["indexed_blobs"]  # JSON list
    assert state["embedding_model"] == "text-embedding-3-large"
    assert state["total_chunks"] == 100
    assert state["total_blobs"] == 3

def test_save_index_state_upsert_pattern(tmp_path, mock_blob_locations):
    """save_index_state() replaces old state (upsert)."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_100, mock_blob_locations)

    # Save state twice
    store.save_index_state("commit1", ["blob1"], "text-embedding-3-large")
    store.save_index_state("commit2", ["blob1", "blob2"], "text-embedding-3-large")

    # Only one row should exist (upsert, not append)
    metadata_df = store.metadata_table.to_pandas()
    states = metadata_df[metadata_df["key"] == "index_state"]
    assert len(states) == 1
    assert states.iloc[0]["last_commit"] == "commit2"

def test_save_index_state_empty_table_no_exception(tmp_path):
    """save_index_state() handles empty metadata table gracefully."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    # Should not raise exception even if table is empty
    store.save_index_state("commit1", [], "text-embedding-3-large")

    metadata_df = store.metadata_table.to_pandas()
    assert len(metadata_df) == 1
```

**Run tests - they FAIL** (red)

### Step 2: Implement save_index_state (Green) - 30 minutes

```python
# src/gitctx/storage/lancedb_store.py

def save_index_state(
    self,
    last_commit: str,
    indexed_blobs: list[str],
    embedding_model: str
):
    """Save index state metadata.

    Uses upsert pattern: delete old state, insert new state.

    Args:
        last_commit: Git commit SHA from last indexing
        indexed_blobs: List of blob SHAs that were indexed
        embedding_model: Model used for embeddings (e.g., "text-embedding-3-large")
    """
    import json
    import logging
    from datetime import UTC, datetime

    logger = logging.getLogger(__name__)

    # Delete old state (upsert pattern)
    try:
        self.metadata_table.delete("key = 'index_state'")
    except Exception:
        # Table might be empty on first save - that's OK
        pass

    # Insert new state
    state = {
        "key": "index_state",
        "last_commit": last_commit,
        "indexed_blobs": json.dumps(indexed_blobs),  # Store as JSON string
        "last_indexed": datetime.now(UTC).isoformat(),
        "embedding_model": embedding_model,
        "total_chunks": self.count(),
        "total_blobs": len(indexed_blobs),
    }

    self.metadata_table.add([state])
    logger.info(f"Saved index state: {len(indexed_blobs)} blobs indexed at {last_commit[:8]}")
```

**Run tests - they PASS** (green)

### Step 3: Write Integration Tests FIRST (Red) - 30 minutes

```python
# tests/unit/storage/test_lancedb_store.py

def test_query_returns_complete_blob_location_context(tmp_path, mock_embeddings, mock_blob_locations):
    """Query results include all 11 BlobLocation fields."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings, mock_blob_locations)

    query_vector = [0.1] * 3072
    results = store.search(query_vector, limit=5)

    # Verify complete BlobLocation context (no joins needed)
    first = results[0]
    assert "blob_sha" in first
    assert "file_path" in first
    assert "start_line" in first
    assert "end_line" in first
    assert "commit_sha" in first
    assert "author_name" in first
    assert "author_email" in first
    assert "commit_date" in first
    assert "commit_message" in first
    assert "is_head" in first
    assert "is_merge" in first

def test_statistics_language_breakdown(tmp_path, mock_embeddings_multilang):
    """get_statistics() returns language counts."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    # Mock: 50 python, 30 javascript, 20 go chunks
    store.add_chunks_batch(mock_embeddings_multilang, mock_blob_locations)

    stats = store.get_statistics()

    assert "languages" in stats
    langs = stats["languages"]
    assert langs["python"] == 50
    assert langs["javascript"] == 30
    assert langs["go"] == 20

@pytest.mark.integration
def test_integration_with_blob_location_data(tmp_path, e2e_git_repo_factory):
    """Integration test with actual BlobRecord from walker."""
    from gitctx.core.commit_walker import CommitWalker
    from gitctx.core.chunker import LanguageAwareChunker
    from gitctx.embeddings.openai_embedder import OpenAIEmbedder
    from gitctx.embeddings.embedding_cache import EmbeddingCache

    # Create real git repo with commits
    repo_path = e2e_git_repo_factory(num_commits=3, files={"test.py": "def foo(): pass\n" * 20})

    # Real components
    walker = CommitWalker(repo_path, config=test_config)
    chunker = LanguageAwareChunker()
    cache = EmbeddingCache(tmp_path / ".gitctx", "text-embedding-3-large", "3072")
    embedder = OpenAIEmbedder(api_key="test-key", cache=cache)  # Will use cache, not real API
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    # Walk and collect blob locations
    all_blob_locations = {}
    for blob_record in walker.walk():
        all_blob_locations[blob_record.sha] = blob_record.locations

        # Chunk
        chunks = chunker.chunk_file(
            content=blob_record.content.decode("utf-8"),
            language="python",
            max_tokens=800
        )

        # Embed (will use cache, not call API)
        # ... (mock or cache embedding generation)

        # Store with denormalized BlobLocation
        store.add_chunks_batch(embeddings, {blob_record.sha: blob_record.locations})

    # Verify
    assert store.count() > 0
    stats = store.get_statistics()
    assert stats["total_blobs"] > 0
```

**Run tests - they FAIL** (red)

### Step 4: Complete Integration (Green) - 40 minutes

Verify all methods work correctly with real walker data structures.

**Run tests - they PASS** (green)

### Step 5: Implement Final BDD Steps - 40 minutes

Update `tests/e2e/steps/lancedb_storage_steps.py` to complete:

**Scenario 5: Index state tracking**
```python
@when(parsers.parse("I save index metadata"))
def save_index_metadata(context):
    context["store"].save_index_state(
        last_commit="abc123def",
        indexed_blobs=context["indexed_blobs"],
        embedding_model="text-embedding-3-large"
    )

@then(parsers.parse("metadata should include: last_commit = {commit}"))
def verify_last_commit(context, commit):
    metadata_df = context["store"].metadata_table.to_pandas()
    state = metadata_df[metadata_df["key"] == "index_state"].iloc[0]
    assert state["last_commit"] == commit
```

**Scenario 7: Statistics reporting**
```python
@when("I request statistics")
def request_statistics(context):
    context["stats"] = context["store"].get_statistics()

@then(parsers.parse("total_chunks should be {count:d}"))
def verify_total_chunks(context, count):
    assert context["stats"]["total_chunks"] == count
```

**Scenario 8: Query with blob location context**
```python
@when("I search for similar vectors")
def search_similar_vectors(context):
    query_vector = [0.1] * 3072
    context["results"] = context["store"].search(query_vector, limit=10)

@then("results should include complete context without joins")
def verify_complete_context(context):
    result = context["results"][0]
    required_fields = [
        "chunk_content", "file_path", "start_line", "end_line",
        "blob_sha", "commit_sha", "author_name", "commit_date",
        "commit_message", "is_head"
    ]
    for field in required_fields:
        assert field in result, f"Missing field: {field}"
```

**Run BDD tests**:
```bash
uv run pytest tests/e2e/features/lancedb_storage.feature -v
# Expected: 10/10 scenarios passing âœ…
```

### Step 6: Verify Performance Targets - 20 minutes

```python
@pytest.mark.slow
def test_performance_insertion_speed(tmp_path, mock_embeddings_10000):
    """Verify >300 chunks/sec insertion speed."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")

    import os
    import time
    start = time.time()
    store.add_chunks_batch(mock_embeddings_10000, mock_blob_locations)
    elapsed = time.time() - start

    # Configurable threshold for different hardware
    min_chunks_per_sec = int(os.getenv("GITCTX_PERF_THRESHOLD_FAST", "300"))
    chunks_per_sec = 10000 / elapsed
    assert chunks_per_sec >= min_chunks_per_sec, f"Insertion too slow: {chunks_per_sec:.1f} chunks/sec (target: {min_chunks_per_sec}+)"

@pytest.mark.slow
def test_performance_search_latency(tmp_path, mock_embeddings_1000):
    """Verify <100ms search latency with IVF-PQ index."""
    store = LanceDBStore(tmp_path / ".gitctx" / "lancedb")
    store.add_chunks_batch(mock_embeddings_1000, mock_blob_locations)
    store.optimize()  # Create IVF-PQ index

    query_vector = [0.1] * 3072

    import os
    import time
    start = time.time()
    results = store.search(query_vector, limit=10)
    elapsed = time.time() - start

    # Configurable threshold for different hardware
    max_latency_ms = int(os.getenv("GITCTX_SEARCH_LATENCY_MS", "100"))
    latency_ms = elapsed * 1000
    assert latency_ms < max_latency_ms, f"Search too slow: {latency_ms:.1f}ms (target: <{max_latency_ms}ms)"
```

## Unit Tests (Write FIRST - TDD)

### New tests in test_lancedb_store.py (7 tests)

1. `test_save_index_state_stores_metadata` - All 6 fields stored correctly
2. `test_save_index_state_upsert_pattern` - Only one state row exists after multiple saves
3. `test_save_index_state_empty_table_no_exception` - First save doesn't raise
4. `test_query_returns_complete_blob_location_context` - All 11 BlobLocation fields present
5. `test_statistics_language_breakdown` - Languages dict with counts
6. `test_performance_insertion_speed` - >300 chunks/sec (@pytest.mark.slow)
7. `test_performance_search_latency` - <100ms with IVF-PQ (@pytest.mark.slow)

### Integration test (1 test)

8. `test_integration_with_blob_location_data` - Real walker â†’ chunker â†’ embedder â†’ store (@pytest.mark.integration)

## BDD Scenarios (10/10 Passing âœ…)

**Scenario 5: Index state tracking** âœ…
- Save index metadata with commit, blobs, model
- Verify metadata includes all fields
- Verify timestamps are ISO format

**Scenario 7: Statistics reporting** âœ…
- Index 5000 chunks from 100 files
- Request statistics
- Verify: total_chunks=5000, total_files=100, total_blobs, languages dict, index_size_mb

**Scenario 8: Query with blob location context** âœ…
- Search for similar vectors
- Verify results include complete context (11 BlobLocation fields)
- Verify no joins needed (all data in single query)

**All other scenarios** (from TASK-2, TASK-3) âœ…

## Coverage Verification

Run coverage report and ensure >90% for all storage module files:
```bash
uv run pytest tests/unit/storage/ --cov=src/gitctx/storage --cov-report=term-missing --cov-fail-under=90

# Expected: >90% coverage for:
# - schema.py
# - protocols.py
# - lancedb_store.py

# If coverage < 90%:
# 1. Review term-missing output to identify uncovered lines
# 2. Add unit tests for uncovered code paths
# 3. Re-run coverage check
# 4. Repeat until all files â‰¥90%
```

## Dependencies

**Requires**:
- TASK-0001.2.4.3 complete (core operations)
- Real walker output for integration test (STORY-0001.2.1)

**Blocks**:
- EPIC-0001.3 (Vector Search) - will use this storage

## Performance Targets (Final Verification)

**Hardware Baseline**: MacBook Pro M1, 16GB RAM, SSD

- **Insertion speed**: >300 chunks/second âœ…
- **Search latency**: <100ms for top-10 results (with IVF-PQ) âœ…
- **Index size**: ~150MB for 10K chunks (3072-dim)
- **Denormalization overhead**: <5% vs normalized

**Hardware Variability**: Configure thresholds via environment variables:
- `GITCTX_PERF_THRESHOLD_FAST`: Minimum insertion speed in chunks/sec (default: 300)
- `GITCTX_SEARCH_LATENCY_MS`: Maximum search latency in ms (default: 100)

## Success Criteria

Task complete when:

- âœ… `save_index_state()` implemented with upsert pattern
- âœ… Metadata table operations handle empty table gracefully
- âœ… Query results include complete BlobLocation context (11 fields)
- âœ… Statistics include languages dict
- âœ… Full E2E integration test passes with real walker output
- âœ… All 7 new unit tests pass (3 save_index_state tests + 3 query/stats tests + 1 E2E integration test, TDD workflow followed)
- âœ… All 10 BDD scenarios pass
- âœ… Code coverage >90% for storage module
- âœ… Performance targets met: >300 chunks/sec, <100ms search
- âœ… Quality gates pass: ruff, mypy, pytest

## Future Enhancements (Post-Story)

These items are NOT required for story completion but should be considered for future work:

1. **Concurrent access control**: File-based locking for multi-process safety
2. **Disk space validation**: Check available space before batch insert
3. **Database corruption recovery**: Detect and handle corrupted LanceDB files
4. **Search recall metrics**: Verify IVF-PQ maintains >95% recall vs exact search

## Notes

**Upsert Pattern**: Delete old state before inserting new to ensure only one state row exists. Handle empty table gracefully (delete may fail on first save - that's expected).

**Complete Context**: Query results include all 11 BlobLocation fields plus chunk content and metadata. This demonstrates the value of denormalization - no joins needed, single query gets everything.

**Performance Verification**: Final performance tests confirm targets met. Mark with `@pytest.mark.slow` to skip in fast test runs. Use realistic data sizes (1000-10000 chunks) to validate production performance.

**Integration Testing**: Use real `BlobRecord` from walker to verify denormalization logic works with actual data structures. This catches integration issues that unit tests with mocks would miss.

---

**Created**: 2025-10-08
**Last Updated**: 2025-10-08
