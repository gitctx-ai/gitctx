# TASK-0001.2.6.2: Enable Embedding Cache in Pipeline

**Parent**: [STORY-0001.2.6](README.md)
**Status**: ðŸ”µ Not Started
**Estimated Hours**: 3
**Actual Hours**: _TBD_

## Objective

Replace manual chunking/embedding logic in pipeline with `embed_with_cache()` orchestration to enable safetensor caching and eliminate redundant API calls.

**Root Cause**: Pipeline calls `embedder.embed_chunks()` directly, bypassing the cache layer. No `.gitctx/embeddings/*.safetensors` files created.

**Solution**: Use existing `embed_with_cache()` orchestration that handles chunk â†’ embed â†’ cache workflow.

## Implementation Checklist

### 1. Add Imports

**File**: `src/gitctx/indexing/pipeline.py` (around line 50)

- [ ] Add cache orchestration imports:
  ```python
  from gitctx.indexing.embeddings import embed_with_cache
  from gitctx.storage.embedding_cache import EmbeddingCache
  ```

### 2. Initialize Cache

**File**: `src/gitctx/indexing/pipeline.py` (around line 62, after chunker/embedder init)

- [ ] Create cache instance:
  ```python
  cache = EmbeddingCache(
      cache_dir=repo_path / ".gitctx",
      model="text-embedding-3-large"
  )
  ```

### 3. Replace Manual Logic with Orchestration

**File**: `src/gitctx/indexing/pipeline.py` (lines ~101-156)

- [ ] **REMOVE** ~50 lines of manual processing (lines 101-156 approximately in `src/gitctx/indexing/pipeline.py`):
  ```python
  # DELETE THESE LINES (approximate range 101-156):
  # - content.decode("utf-8") handling with try/except
  # - language detection via detect_language()
  # - chunker.chunk_file() call in loop
  # - embedder.embed_chunks() API call
  # - Manual Embedding() object construction from response
  # - Token/cost tracking from embedder.usage response
  #
  # NOTE: Exact lines may vary - remove the entire manual embedding workflow
  # from blob iteration start to store.add_chunks_batch() call
  ```

- [ ] **REPLACE** with orchestrated call (~10 lines):
  ```python
  for blob_record in blob_records:
      try:
          # Single orchestrated call: check cache â†’ chunk â†’ embed â†’ save cache
          embeddings = await embed_with_cache(
              chunker=chunker,
              embedder=embedder,
              cache=cache,
              blob_record=blob_record,
          )

          # Track stats (embeddings already have all metadata)
          total_tokens = sum(e.token_count for e in embeddings)
          total_cost = sum(e.cost_usd for e in embeddings)
          reporter.update(tokens=total_tokens, cost=total_cost, chunks=len(embeddings))

          # Store (embeddings have all fields: chunk_content, vectors, metadata)
          blob_locations = {blob_record.sha: blob_record.locations}
          store.add_chunks_batch(embeddings=embeddings, blob_locations=blob_locations)

      except Exception as e:
          logger.error(f"Failed to process blob {blob_record.sha[:7]}: {e}")
          continue
  ```

### 4. Remove Redundant Error Handling

**File**: `src/gitctx/indexing/pipeline.py`

- [ ] Remove UTF-8 decode error handling (now in `embed_with_cache()`)
- [ ] Remove language detection code (now in `embed_with_cache()`)
- [ ] Simplify exception handling (single try/catch per blob)

### 5. Update Type Hints

**File**: `src/gitctx/indexing/pipeline.py`

- [ ] Ensure function signatures match `embed_with_cache()` expectations
- [ ] Verify `Embedding` type is imported: `from gitctx.embeddings.types import Embedding`

### 6. Unit Tests

**File**: `tests/unit/storage/test_embedding_cache.py`

- [ ] Test cache creates safetensor files:
  ```python
  def test_cache_creates_safetensor_files(tmp_path: Path):
      """Cache should create .safetensors files on set()."""
      cache = EmbeddingCache(tmp_path, model="text-embedding-3-large")

      embedding = Embedding(
          vector=np.random.rand(3072),
          blob_sha="abc123",
          chunk_index=0,
          # ... other fields
      )

      cache.set(blob_sha="abc123", chunk_index=0, embedding=embedding)

      # Verify file created
      cache_file = tmp_path / "embeddings" / "text-embedding-3-large" / "abc123_chunk_0.safetensors"
      assert cache_file.exists()
  ```

- [ ] Test cache hit returns embeddings:
  ```python
  def test_cache_hit_returns_embeddings(tmp_path: Path):
      """Cache should return stored embeddings on get()."""
      cache = EmbeddingCache(tmp_path, model="text-embedding-3-large")

      # Store
      original = Embedding(vector=np.random.rand(3072), ...)
      cache.set("abc123", 0, original)

      # Retrieve
      retrieved = cache.get("abc123", 0)

      assert retrieved is not None
      np.testing.assert_array_equal(retrieved.vector, original.vector)
  ```

### 7. Integration Test

**File**: `tests/unit/indexing/test_pipeline.py`

- [ ] Test pipeline creates cache files:
  ```python
  async def test_pipeline_creates_embedding_cache(
      tmp_git_repo: Path,
      mock_embedder: MockEmbedder
  ):
      """Pipeline should create safetensor cache files."""
      await index_repository(tmp_git_repo, embedder=mock_embedder)

      # Verify cache directory exists
      cache_dir = tmp_git_repo / ".gitctx" / "embeddings" / "text-embedding-3-large"
      assert cache_dir.exists()

      # Verify at least one cache file created
      cache_files = list(cache_dir.glob("*.safetensors"))
      assert len(cache_files) > 0
  ```

- [ ] Test second run uses cache (no API calls):
  ```python
  async def test_pipeline_uses_cache_on_second_run(
      tmp_git_repo: Path,
      mock_embedder: MockEmbedder
  ):
      """Second run should use cache, make no API calls."""
      # First run
      await index_repository(tmp_git_repo, embedder=mock_embedder)
      first_run_calls = mock_embedder.call_count

      # Second run
      await index_repository(tmp_git_repo, embedder=mock_embedder)
      second_run_calls = mock_embedder.call_count

      # No new API calls on second run (100% cache hit)
      assert second_run_calls == first_run_calls
  ```

## Verification

### Run Tests
```bash
# Unit tests
uv run pytest tests/unit/storage/test_embedding_cache.py -v
uv run pytest tests/unit/indexing/test_pipeline.py::test_pipeline_creates_embedding_cache -v
uv run pytest tests/unit/indexing/test_pipeline.py::test_pipeline_uses_cache_on_second_run -v

# All tests
uv run pytest
```

### Manual Testing
```bash
# Clean slate
rm -rf .gitctx/

# First run - should create cache files
gitctx index --verbose
# Expected: "Embedding chunk..." messages, cost > $0

# Verify cache files created
ls -lh .gitctx/embeddings/text-embedding-3-large/
# Should see *.safetensors files

# Second run - should use cache
gitctx index --verbose
# Expected: "Cache hit for blob abc123" messages, cost = $0.00
```

### Expected Output

**First run:**
```
âœ“ Indexing Complete
  Blobs:   314
  Chunks:  1,247
  Tokens:  342,891
  Cost:    $3.24
  Time:    45.2s
```

**Second run (with cache):**
```
âœ“ Indexing Complete
  Blobs:   314
  Chunks:  1,247
  Tokens:  0 (cached)
  Cost:    $0.00
  Time:    2.1s
```

## Files Modified

| File | Lines Added | Lines Removed | Net Change | Purpose |
|------|-------------|---------------|------------|---------|
| `src/gitctx/indexing/pipeline.py` | +15 | -50 | **-35** | Enable caching |
| `tests/unit/storage/test_embedding_cache.py` | +30 | 0 | +30 | Cache tests |
| `tests/unit/indexing/test_pipeline.py` | +25 | 0 | +25 | Integration tests |

**Total**: +70 added, -50 removed = **+20 net** (major simplification in pipeline)

## Success Criteria

- [ ] Pipeline imports and initializes `EmbeddingCache`
- [ ] Pipeline calls `embed_with_cache()` instead of direct embedder
- [ ] First run creates `.gitctx/embeddings/**/*.safetensors` files
- [ ] Second run shows 100% cache hit rate, $0.00 cost
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] Type checking passes (`uv run mypy src`)
- [ ] Linting passes (`uv run ruff check src tests`)

## Benefits

**Code Simplification:**
- **-35 net lines** in pipeline (remove ~50, add ~15)
- Single responsibility: pipeline orchestrates, cache handles storage
- Easier to test and maintain

**Performance:**
- First run: Same cost (~$3)
- Second run: **FREE** ($0.00, 100% cache hit)
- 20x faster (2s vs 45s)

**User Experience:**
- Transparent caching (works automatically)
- Can delete cache anytime (`rm -rf .gitctx/embeddings`)
- Clear cache hit messages in `--verbose` mode

## Related Tasks

- **TASK-0001.2.6.1**: Snapshot mode reduces blobs to cache
- **TASK-0001.2.6.3**: Cost estimator will note "excluding cache hits"
- **TASK-0001.2.6.4**: History mode warning mentions cache benefits

## Notes

- **No behavior change** for first run (same cost, same results)
- **Major savings** on subsequent runs (100% free)
- Cache stored per model (`text-embedding-3-large/`)
- Cache key: `{blob_sha}_{chunk_index}.safetensors`
- Safe to delete cache (will regenerate on next run)

---

**Created**: 2025-10-15
