# EPIC-0002.1: Intelligent Context Assembly

**Parent Initiative**: [INIT-0002](../README.md)  
**Status**: ðŸ”µ Not Started  
**Estimated**: 21 story points  
**Progress**: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0%

## Overview

Enhance basic search with multi-source context aggregation and GPT-5-Codex reranking. Combine results from code, tests, and git history to provide comprehensive context for coding tasks.

## Goals

- Aggregate context from multiple sources (code, tests, git history)
- Use GPT-5-Codex to rerank by relevance
- Manage token budgets effectively  
- Synthesize coherent context
- Provide source attribution

## Child Stories

| ID | Title | Status | Points |
|----|-------|--------|--------|
| [STORY-0002.1.1](../stories/STORY-0002.1.1.md) | Multi-Source Aggregation | ðŸ”µ Not Started | 5 |
| [STORY-0002.1.2](../stories/STORY-0002.1.2.md) | GPT-5-Codex Reranking | ðŸ”µ Not Started | 5 |
| [STORY-0002.1.3](../stories/STORY-0002.1.3.md) | Token Budget Management | ðŸ”µ Not Started | 3 |
| [STORY-0002.1.4](../stories/STORY-0002.1.4.md) | Context Synthesis | ðŸ”µ Not Started | 5 |
| [STORY-0002.1.5](../stories/STORY-0002.1.5.md) | Source Attribution | ðŸ”µ Not Started | 3 |

## BDD Specifications

```gherkin
# tests/e2e/features/context.feature

Feature: Context Assembly
  As a developer
  I want comprehensive context for my queries
  So that I understand the complete codebase

  Background:
    Given an indexed repository with tests

  Scenario: Multi-source results
    When I run "gitctx search 'user authentication'"
    Then results should include implementation files
    And results should include test files
    And results should include recent commits
    And each result should show its source type

  Scenario: GPT-5-Codex reranking
    Given search results for "database connection"
    When results are reranked by GPT-5-Codex
    Then most relevant code should appear first
    And relevance scores should be visible
    And semantic density should exceed 0.6

  Scenario: Token budget respected
    When I run "gitctx search 'complex query' --max-tokens 4000"
    Then total tokens should not exceed 4000
    And results should include diverse sources
    And truncation should be indicated

  Scenario: Context synthesis
    When I run "gitctx search 'api endpoint' --synthesize"
    Then results should be coherently organized
    And related code should be grouped
    And transitions should be clear

  Scenario: Source attribution
    When I run "gitctx search 'error handling'"
    Then each result should show file path
    And each result should show line numbers
    And test files should be labeled as tests
    And git commits should show hash and message
```

## Technical Design

### Multi-Source Aggregator

```python
# src/gitctx/context/aggregator.py
from typing import List, Dict
import git

class ContextAggregator:
    def __init__(self, repo_path: Path):
        self.repo = git.Repo(repo_path)
        self.search_engine = SearchEngine()
    
    def aggregate_context(self, query: str) -> List[ContextChunk]:
        """Gather context from multiple sources."""
        results = []
        
        # Search code files
        code_results = self.search_engine.search(query)
        results.extend(self._label_results(code_results, "code"))
        
        # Find related tests
        test_results = self._find_related_tests(code_results)
        results.extend(self._label_results(test_results, "test"))
        
        # Search git history
        git_results = self._search_git_history(query)
        results.extend(self._label_results(git_results, "history"))
        
        # Deduplicate and merge
        return self._merge_results(results)
    
    def _find_related_tests(self, code_results):
        """Find test files related to code results."""
        # Look for test_ prefixes
        # Check test directories
        # Match function names
```

### GPT-5-Codex Reranker

```python
# src/gitctx/search/reranker.py
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate

class Reranker:
    def __init__(self):
        self.llm = OpenAI(
            model="gpt-5-codex",
            temperature=0
        )
        self.prompt = PromptTemplate(
            template="""Score the relevance of this code to the query.
            Query: {query}
            Code: {code}
            Score (0-10):"""
        )
    
    async def rerank(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """Rerank results using GPT-5-Codex."""
        # Batch results for efficiency
        batches = self._create_batches(results, batch_size=20)
        
        scores = []
        for batch in batches:
            batch_scores = await self._score_batch(query, batch)
            scores.extend(batch_scores)
        
        # Sort by score
        return sorted(
            zip(results, scores),
            key=lambda x: x[1],
            reverse=True
        )
```

### Token Manager

```python
# src/gitctx/context/token_manager.py
import tiktoken

class TokenManager:
    def __init__(self, max_tokens: int = 8000):
        self.max_tokens = max_tokens
        self.encoder = tiktoken.encoding_for_model("gpt-4")
    
    def fit_to_budget(self, chunks: List[ContextChunk]) -> List[ContextChunk]:
        """Fit chunks within token budget."""
        selected = []
        total_tokens = 0
        
        # Ensure diversity
        by_type = self._group_by_type(chunks)
        
        # Round-robin selection
        while total_tokens < self.max_tokens:
            for chunk_type in by_type:
                if by_type[chunk_type]:
                    chunk = by_type[chunk_type].pop(0)
                    chunk_tokens = self._count_tokens(chunk)
                    
                    if total_tokens + chunk_tokens <= self.max_tokens:
                        selected.append(chunk)
                        total_tokens += chunk_tokens
        
        return selected
```

### Context Synthesizer

```python
# src/gitctx/context/synthesizer.py
class ContextSynthesizer:
    def __init__(self):
        self.llm = OpenAI(model="gpt-5-codex")
    
    def synthesize(self, chunks: List[ContextChunk], query: str) -> str:
        """Create coherent context from chunks."""
        # Group related chunks
        grouped = self._group_related(chunks)
        
        # Build context document
        context = []
        for group in grouped:
            context.append(self._format_group(group))
        
        # Optional: Use LLM to synthesize
        if use_llm_synthesis:
            return self._llm_synthesize(context, query)
        
        return "\n\n".join(context)
    
    def _format_group(self, chunks: List[ContextChunk]) -> str:
        """Format a group of related chunks."""
        header = f"## {chunks[0].file_path}\n"
        body = "\n".join(chunk.content for chunk in chunks)
        return header + body
```

## Dependencies

- `langchain` - LLM integration for reranking
- `tiktoken` - Token counting for budget management
- `gitpython` - Git history access
- `numpy` - Score calculations

## Success Criteria

1. **Multi-source context** - Code + tests + history aggregated
2. **Intelligent ranking** - GPT-5-Codex reranking improves relevance
3. **Token awareness** - Stays within configured limits
4. **Source diversity** - >60% queries have multiple source types
5. **Clear attribution** - Every chunk traceable to source

## Performance Targets

- Reranking latency <1s additional
- Semantic density >0.6
- Source diversity >60%
- Token utilization >80%
- Cost per query <$0.05

## Notes

- Multi-source aggregation is key differentiator
- GPT-5-Codex reranking significantly improves relevance
- Token management ensures LLM compatibility
- Source attribution enables navigation
- This epic transforms search into intelligent context assembly

---

**Created**: 2025-09-28  
**Last Updated**: 2025-09-28
